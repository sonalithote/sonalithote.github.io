<!DOCTYPE html>
<html lang="en-GB">

  <head>
    <!-- General meta -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    

    
      <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Traffic Signs Detection | Alembic</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Traffic Signs Detection" />
<meta name="author" content="sonalithote" />
<meta property="og:locale" content="en_GB" />
<meta name="description" content="The primary objectives for any recognition system include detection (ascertaining the location and size of an object within an input image) and classification (assigning the detected objects into specific subclasses). Typically, a singular detection/classification model, such as YOLO or SSD, is employed for both tasks, where input images are annotated with bounding boxes and their corresponding classes. However, the labelling and training of such datasets demand considerable time and effort. Consequently, the principal aim of this project is to identify a single main class (signs) and to incorporate a custom-built Convolutional Neural Network for the classification of the detected objects into subclasses (for instance, speed limits, stop signs, etc.). This approach necessitates training a detection model only once to recognize one main class, while allowing for the training of multiple classification models to categorize detected objects based on the specific requirements of the task." />
<meta property="og:description" content="The primary objectives for any recognition system include detection (ascertaining the location and size of an object within an input image) and classification (assigning the detected objects into specific subclasses). Typically, a singular detection/classification model, such as YOLO or SSD, is employed for both tasks, where input images are annotated with bounding boxes and their corresponding classes. However, the labelling and training of such datasets demand considerable time and effort. Consequently, the principal aim of this project is to identify a single main class (signs) and to incorporate a custom-built Convolutional Neural Network for the classification of the detected objects into subclasses (for instance, speed limits, stop signs, etc.). This approach necessitates training a detection model only once to recognize one main class, while allowing for the training of multiple classification models to categorize detected objects based on the specific requirements of the task." />
<link rel="canonical" href="http://localhost:4000/ml_project/2021/03/22/traffic-signs-detection/" />
<meta property="og:url" content="http://localhost:4000/ml_project/2021/03/22/traffic-signs-detection/" />
<meta property="og:site_name" content="Alembic" />
<meta property="og:image" content="https://picsum.photos/2560/600?image=733" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-22T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://picsum.photos/2560/600?image=733" />
<meta property="twitter:title" content="Traffic Signs Detection" />
<meta name="twitter:site" content="@sonalithote" />
<meta name="twitter:creator" content="@sonalithote" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"sonalithote"},"url":"http://localhost:4000/ml_project/2021/03/22/traffic-signs-detection/","image":"https://picsum.photos/2560/600?image=733","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/logos/logo.svg"},"name":"sonalithote"},"headline":"Traffic Signs Detection","dateModified":"2021-03-22T00:00:00+00:00","datePublished":"2021-03-22T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/ml_project/2021/03/22/traffic-signs-detection/"},"description":"The primary objectives for any recognition system include detection (ascertaining the location and size of an object within an input image) and classification (assigning the detected objects into specific subclasses). Typically, a singular detection/classification model, such as YOLO or SSD, is employed for both tasks, where input images are annotated with bounding boxes and their corresponding classes. However, the labelling and training of such datasets demand considerable time and effort. Consequently, the principal aim of this project is to identify a single main class (signs) and to incorporate a custom-built Convolutional Neural Network for the classification of the detected objects into subclasses (for instance, speed limits, stop signs, etc.). This approach necessitates training a detection model only once to recognize one main class, while allowing for the training of multiple classification models to categorize detected objects based on the specific requirements of the task.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
    
      
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
      
    

    <link rel="manifest" href="/manifest.json">
    <meta name="theme-color" content="#242e2b"/>

    
      
<style>/*! normalize.css v4.1.1 | MIT License | github.com/necolas/normalize.css */html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block}audio:not([controls]){display:none;height:0}progress{vertical-align:baseline}template,[hidden]{display:none}a{background-color:rgba(0,0,0,0);-webkit-text-decoration-skip:objects}a:active,a:hover{outline-width:0}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:inherit}b,strong{font-weight:bolder}dfn{font-style:italic}h1{font-size:2em;margin:.67em 0}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-0.25em}sup{top:-0.5em}img{border-style:none}svg:not(:root){overflow:hidden}code,kbd,pre,samp{font-family:monospace,monospace;font-size:1em}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}button,input,select,textarea{font:inherit;margin:0}optgroup{font-weight:bold}button,input{overflow:visible}button,select{text-transform:none}button,html [type=button],[type=reset],[type=submit]{-webkit-appearance:button}button::-moz-focus-inner,[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner{border-style:none;padding:0}button:-moz-focusring,[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring{outline:1px dotted ButtonText}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-input-placeholder{color:inherit;opacity:.54}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}*{box-sizing:border-box}html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,menu,nav,section{display:block}html,body{height:100%}a img{border:none}blockquote{quotes:none}blockquote:before,blockquote:after{content:"";content:none}table{border-collapse:collapse;border-spacing:0}caption,th,td{text-align:left;font-weight:normal;vertical-align:middle}html{font-size:75%;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%}@media screen and (min-width: 40em){html{font-size:87.5%}}@media screen and (min-width: 50em){html{font-size:93.75%}}@media screen and (min-width: 64em){html{font-size:106.25%}}@media screen and (min-width: 100em){html{font-size:118.75%}}body{font-size:1.3333333333rem;font-family:Georgia, serif;font-style:normal;font-weight:400;line-height:2rem}@media screen and (min-width: 40em){body{font-size:1.2142857143rem}}@media screen and (min-width: 50em){body{font-size:1.2rem}}@media screen and (min-width: 64em){body{font-size:1.1764705882rem}}@media screen and (min-width: 100em){body{font-size:1.1578947368rem}}a{color:#05bf85;text-decoration:none;transition:color .1s,background-color .1s}a:hover,a:active,a:focus{color:green;text-decoration:none}.typeset p a,.typeset li a{background-image:linear-gradient(to bottom, rgba(0, 0, 0, 0) 50%, #05bf85 50%);background-position:0 93%;background-repeat:repeat-x;background-size:100% .15rem;text-shadow:.1rem 0 #fff,.15rem 0 #fff,-0.1rem 0 #fff,-0.15rem 0 #fff}.typeset p a:hover,.typeset p a:active,.typeset p a:focus,.typeset li a:hover,.typeset li a:active,.typeset li a:focus{background-image:linear-gradient(to bottom, rgba(0, 0, 0, 0) 50%, #008000 50%)}.typeset p{line-height:2rem;margin-bottom:1.49999rem;padding-top:0.50001rem;font-feature-settings:"kern","onum","liga"}@media screen and (min-width: 40em){.typeset p{margin-bottom:1.4553471429rem;padding-top:0.5446528571rem}}@media screen and (min-width: 50em){.typeset p{margin-bottom:1.44999rem;padding-top:0.55001rem}}@media screen and (min-width: 64em){.typeset p{margin-bottom:1.4411664706rem;padding-top:0.5588335294rem}}@media screen and (min-width: 100em){.typeset p{margin-bottom:1.4342005263rem;padding-top:0.5657994737rem}}.typeset h1,.typeset h2,.typeset h3,.typeset h4,.typeset h5,.typeset h6{color:#242e2b;font-family:"Merriweather", serif;font-feature-settings:"dlig","liga","lnum","kern";font-style:normal;font-weight:700}.typeset h1,.typeset .alpha{line-height:3rem;font-size:2.3333333333rem;margin-bottom:0.37499rem;padding-top:0.62501rem}@media screen and (min-width: 40em){.typeset h1,.typeset .alpha{font-size:2.1428571429rem;margin-bottom:0.3035614286rem;padding-top:0.6964385714rem}}@media screen and (min-width: 50em){.typeset h1,.typeset .alpha{font-size:2.1333333333rem;margin-bottom:0.29999rem;padding-top:0.70001rem}}@media screen and (min-width: 64em){.typeset h1,.typeset .alpha{font-size:2.3529411765rem;margin-bottom:0.3823429412rem;padding-top:0.6176570588rem}}@media screen and (min-width: 100em){.typeset h1,.typeset .alpha{font-size:2.6315789474rem;margin-bottom:0.4868321053rem;padding-top:0.5131678947rem}}.typeset h2,.typeset .beta{line-height:3rem;font-size:2.0833333333rem;margin-bottom:0.28124rem;padding-top:0.71876rem}@media screen and (min-width: 40em){.typeset h2,.typeset .beta{font-size:1.9rem;margin-bottom:0.21249rem;padding-top:0.78751rem}}@media screen and (min-width: 50em){.typeset h2,.typeset .beta{font-size:1.8733333333rem;margin-bottom:0.20249rem;padding-top:0.79751rem}}@media screen and (min-width: 64em){.typeset h2,.typeset .beta{font-size:2.0882352941rem;margin-bottom:0.2830782353rem;padding-top:0.7169217647rem}}@media screen and (min-width: 100em){.typeset h2,.typeset .beta{font-size:2.3105263158rem;margin-bottom:0.3664373684rem;padding-top:0.6335626316rem}}.typeset h3,.typeset .gamma{line-height:3rem;font-size:1.8666666667rem;margin-bottom:0.19999rem;padding-top:0.80001rem}@media screen and (min-width: 40em){.typeset h3,.typeset .gamma{font-size:1.7142857143rem;margin-bottom:0.1428471429rem;padding-top:0.8571528571rem}}@media screen and (min-width: 50em){.typeset h3,.typeset .gamma{font-size:1.7066666667rem;margin-bottom:0.13999rem;padding-top:0.86001rem}}@media screen and (min-width: 64em){.typeset h3,.typeset .gamma{font-size:1.7647058824rem;margin-bottom:0.1617547059rem;padding-top:0.8382452941rem}}@media screen and (min-width: 100em){.typeset h3,.typeset .gamma{font-size:1.8578947368rem;margin-bottom:0.1967005263rem;padding-top:0.8032994737rem}}.typeset h4,.typeset .delta{line-height:2rem;font-size:1.6666666667rem;margin-bottom:-0.37501rem;padding-top:0.37501rem}@media screen and (min-width: 40em){.typeset h4,.typeset .delta{font-size:1.5214285714rem;margin-bottom:-0.4294742857rem;padding-top:0.4294742857rem}}@media screen and (min-width: 50em){.typeset h4,.typeset .delta{font-size:1.5rem;margin-bottom:-0.43751rem;padding-top:0.43751rem}}@media screen and (min-width: 64em){.typeset h4,.typeset .delta{font-size:1.5705882353rem;margin-bottom:-0.4110394118rem;padding-top:0.4110394118rem}}@media screen and (min-width: 100em){.typeset h4,.typeset .delta{font-size:1.6368421053rem;margin-bottom:-0.3861942105rem;padding-top:0.3861942105rem}}.typeset h5,.typeset .epsilon{line-height:2rem;font-size:1.4916666667rem;margin-bottom:-0.440635rem;padding-top:0.440635rem}@media screen and (min-width: 40em){.typeset h5,.typeset .epsilon{font-size:1.3714285714rem;margin-bottom:-0.4857242857rem;padding-top:0.4857242857rem}}@media screen and (min-width: 50em){.typeset h5,.typeset .epsilon{font-size:1.3666666667rem;margin-bottom:-0.48751rem;padding-top:0.48751rem}}@media screen and (min-width: 64em){.typeset h5,.typeset .epsilon{font-size:1.3235294118rem;margin-bottom:-0.5036864706rem;padding-top:0.5036864706rem}}@media screen and (min-width: 100em){.typeset h5,.typeset .epsilon{font-size:1.3157894737rem;margin-bottom:-0.5065889474rem;padding-top:0.5065889474rem}}.typeset h6,.typeset .zeta{line-height:2rem;font-size:1.3333333333rem;margin-bottom:-0.50001rem;padding-top:0.50001rem}@media screen and (min-width: 40em){.typeset h6,.typeset .zeta{font-size:1.2142857143rem;margin-bottom:-0.5446528571rem;padding-top:0.5446528571rem}}@media screen and (min-width: 50em){.typeset h6,.typeset .zeta{font-size:1.2rem;margin-bottom:-0.55001rem;padding-top:0.55001rem}}@media screen and (min-width: 64em){.typeset h6,.typeset .zeta{font-size:1.1764705882rem;margin-bottom:-0.5588335294rem;padding-top:0.5588335294rem}}@media screen and (min-width: 100em){.typeset h6,.typeset .zeta{font-size:1.1578947368rem;margin-bottom:-0.5657994737rem;padding-top:0.5657994737rem}}.typeset ul,.typeset ol{line-height:2rem;margin-bottom:1.49999rem;padding-top:0.50001rem}@media screen and (min-width: 40em){.typeset ul,.typeset ol{margin-bottom:1.4553471429rem;padding-top:0.5446528571rem}}@media screen and (min-width: 50em){.typeset ul,.typeset ol{margin-bottom:1.44999rem;padding-top:0.55001rem}}@media screen and (min-width: 64em){.typeset ul,.typeset ol{margin-bottom:1.4411664706rem;padding-top:0.5588335294rem}}@media screen and (min-width: 100em){.typeset ul,.typeset ol{margin-bottom:1.4342005263rem;padding-top:0.5657994737rem}}.typeset ul li,.typeset ol li{font-feature-settings:"kern","onum","liga";margin-left:2rem}@media screen and (min-width: 40em){.typeset ul li,.typeset ol li{margin-left:0}}.typeset ul li ol,.typeset ul li ul,.typeset ol li ol,.typeset ol li ul{padding-top:1rem;margin-bottom:1rem;margin-left:2rem}.typeset ol{list-style-type:none}.typeset ol li{counter-increment:top-level}.typeset ol li:before{content:counter(top-level) ".";font-feature-settings:"lnum","tnum";margin-left:-3rem;position:absolute;text-align:right;width:2em}.typeset ol li ul li:before{content:""}.typeset ol li ul li ol li{counter-increment:alt-level}.typeset ol li ul li ol li:before{content:counter(alt-level) "."}.typeset ol li ol li{counter-increment:sub-level}.typeset ol li ol li:before{content:counter(top-level) "." counter(sub-level)}.typeset ol li ol li ul li:before{content:""}.typeset ol li ol li ol li{counter-increment:sub-sub-level}.typeset ol li ol li ol li:before{content:counter(top-level) "." counter(sub-level) "." counter(sub-sub-level)}.typeset dl{line-height:2rem;margin-bottom:1.49999rem;padding-top:0.50001rem}@media screen and (min-width: 40em){.typeset dl{margin-bottom:1.4553471429rem;padding-top:0.5446528571rem}}@media screen and (min-width: 50em){.typeset dl{margin-bottom:1.44999rem;padding-top:0.55001rem}}@media screen and (min-width: 64em){.typeset dl{margin-bottom:1.4411664706rem;padding-top:0.5588335294rem}}@media screen and (min-width: 100em){.typeset dl{margin-bottom:1.4342005263rem;padding-top:0.5657994737rem}}.typeset dl dt,.typeset dl dd{font-feature-settings:"kern","onum","liga";margin-left:2rem}@media screen and (min-width: 40em){.typeset dl dt,.typeset dl dd{margin-left:0}}.typeset dl dt{font-weight:700}.typeset dl dd+dt{padding-top:1rem}.typeset table{line-height:2rem;font-size:1.1916666667rem;margin-bottom:-0.553135rem;padding-top:0.553135rem;font-family:"Merriweather", serif;font-feature-settings:"liga","lnum","tnum","kern";font-style:normal;font-weight:400;width:100%}@media screen and (min-width: 40em){.typeset table{font-size:1.0928571429rem;margin-bottom:-0.5901885714rem;padding-top:0.5901885714rem}}@media screen and (min-width: 50em){.typeset table{font-size:1.0866666667rem;margin-bottom:-0.59251rem;padding-top:0.59251rem}}@media screen and (min-width: 64em){.typeset table{font-size:0.9941176471rem;margin-bottom:-0.6272158824rem;padding-top:0.6272158824rem}}@media screen and (min-width: 100em){.typeset table{font-size:0.9263157895rem;margin-bottom:-0.6526415789rem;padding-top:0.6526415789rem}}.typeset table thead th{line-height:2rem;font-size:1.3333333333rem;margin-bottom:-0.50001rem;padding-top:0.50001rem;padding-bottom:1px}@media screen and (min-width: 40em){.typeset table thead th{font-size:1.2142857143rem;margin-bottom:-0.5446528571rem;padding-top:0.5446528571rem}}@media screen and (min-width: 50em){.typeset table thead th{font-size:1.2rem;margin-bottom:-0.55001rem;padding-top:0.55001rem}}@media screen and (min-width: 64em){.typeset table thead th{font-size:1.1764705882rem;margin-bottom:-0.5588335294rem;padding-top:0.5588335294rem}}@media screen and (min-width: 100em){.typeset table thead th{font-size:1.1578947368rem;margin-bottom:-0.5657994737rem;padding-top:0.5657994737rem}}.typeset b,.typeset strong,.typeset .bold{font-weight:700}.typeset em,.typeset i,.typeset .italic{font-style:italic}.typeset small,.typeset .caption{font-size:1.0666666667rem;font-family:"Merriweather", serif;font-style:normal;font-weight:400}@media screen and (min-width: 40em){.typeset small,.typeset .caption{font-size:0.9714285714rem}}@media screen and (min-width: 50em){.typeset small,.typeset .caption{font-size:0.96rem}}@media screen and (min-width: 64em){.typeset small,.typeset .caption{font-size:0.8823529412rem}}@media screen and (min-width: 100em){.typeset small,.typeset .caption{font-size:0.8210526316rem}}.typeset small{line-height:1rem}.typeset .caption{line-height:2rem;margin-bottom:1.39999rem;padding-top:0.60001rem;color:#a8adac}@media screen and (min-width: 40em){.typeset .caption{margin-bottom:1.3642757143rem;padding-top:0.6357242857rem}}@media screen and (min-width: 50em){.typeset .caption{margin-bottom:1.35999rem;padding-top:0.64001rem}}@media screen and (min-width: 64em){.typeset .caption{margin-bottom:1.3308723529rem;padding-top:0.6691276471rem}}@media screen and (min-width: 100em){.typeset .caption{margin-bottom:1.3078847368rem;padding-top:0.6921152632rem}}.typeset h1+.caption,.typeset .alpha+.caption,.typeset h2+.caption,.typeset .beta+.caption,.typeset h3+.caption,.typeset .gamma+.caption{margin-top:-1rem}.typeset .delta+.caption,.typeset .epsilon+.caption,.typeset .zeta+.caption{margin-top:0rem}.typeset blockquote p{border-left:.15rem solid #05bf85;font-style:italic;padding-left:1rem;margin-bottom:1.99999rem;padding-bottom:0.50001rem}@media screen and (min-width: 40em){.typeset blockquote p{margin-bottom:1.9107042857rem;padding-bottom:0.5446528571rem}}@media screen and (min-width: 50em){.typeset blockquote p{margin-bottom:1.89999rem;padding-bottom:0.55001rem}}@media screen and (min-width: 64em){.typeset blockquote p{margin-bottom:1.8823429412rem;padding-bottom:0.5588335294rem}}@media screen and (min-width: 100em){.typeset blockquote p{margin-bottom:1.8684110526rem;padding-bottom:0.5657994737rem}}@media screen and (min-width: 40em){.typeset blockquote{margin-left:-1rem}}.typeset hr{background-image:linear-gradient(to bottom, rgba(0, 0, 0, 0) 50%, #a8adac 50%);background-position:0 50%;background-repeat:repeat-x;background-size:100% .15rem;border:0;margin:0;padding-bottom:3rem;padding-top:3rem}.typeset code,.typeset pre{background-color:#fafafa;font-family:Menlo, monospace}.typeset pre{display:block;margin-bottom:2rem;padding:1rem;white-space:pre;white-space:pre-wrap;word-break:break-all;word-wrap:break-word}.typeset code{font-size:1.0666666667rem;line-height:1rem}@media screen and (min-width: 40em){.typeset code{font-size:0.9714285714rem}}@media screen and (min-width: 50em){.typeset code{font-size:0.96rem}}@media screen and (min-width: 64em){.typeset code{font-size:0.8823529412rem}}@media screen and (min-width: 100em){.typeset code{font-size:0.8210526316rem}}.typeset .upper{font-kerning:normal;letter-spacing:.1rem;text-transform:uppercase}.typeset .small-caps{font-feature-settings:"smcp","kern";font-kerning:normal;letter-spacing:.1rem}.typeset .lining-numerals{font-feature-settings:"lnum","kern"}.typeset .oldstyle-numerals{font-feature-settings:"onum","kern"}.hll{background-color:#ffc}.c{color:#998;font-style:italic}.err{color:#a61717;background-color:#e3d2d2}.k{color:#000;font-weight:bold}.o{color:#000;font-weight:bold}.cm{color:#998;font-style:italic}.cp{color:#999;font-weight:bold;font-style:italic}.c1{color:#998;font-style:italic}.cs{color:#999;font-weight:bold;font-style:italic}.gd{color:#000;background-color:#fdd}.ge{color:#000;font-style:italic}.gr{color:#a00}.gh{color:#999}.gi{color:#000;background-color:#dfd}.go{color:#888}.gp{color:#555}.gs{font-weight:bold}.gu{color:#aaa}.gt{color:#a00}.kc{color:#000;font-weight:bold}.kd{color:#000;font-weight:bold}.kn{color:#000;font-weight:bold}.kp{color:#000;font-weight:bold}.kr{color:#000;font-weight:bold}.kt{color:#458;font-weight:bold}.m{color:#099}.s{color:#d01040}.na{color:teal}.nb{color:#0086b3}.nc{color:#458;font-weight:bold}.no{color:teal}.nd{color:#3c5d5d;font-weight:bold}.ni{color:purple}.ne{color:#900;font-weight:bold}.nf{color:#900;font-weight:bold}.nl{color:#900;font-weight:bold}.nn{color:#555}.nt{color:navy}.nv{color:teal}.ow{color:#000;font-weight:bold}.w{color:#bbb}.mf{color:#099}.mh{color:#099}.mi{color:#099}.mo{color:#099}.sb{color:#d01040}.sc{color:#d01040}.sd{color:#d01040}.s2{color:#d01040}.se{color:#d01040}.sh{color:#d01040}.si{color:#d01040}.sx{color:#d01040}.sr{color:#009926}.s1{color:#d01040}.ss{color:#990073}.bp{color:#999}.vc{color:teal}.vg{color:teal}.vi{color:teal}.il{color:#099}body{background:#fff;color:#384743;height:100%;display:flex;-webkit-box-direction:normal;-webkit-box-orient:vertical;-webkit-flex-direction:column;-moz-flex-direction:column;-ms-flex-direction:column;flex-direction:column;overflow-x:hidden}.container{width:90%;max-width:1200px;margin:0 auto}.header .nav,.footer .nav{text-align:right}.header .logo,.footer .logo{-webkit-box-flex:0;-webkit-flex:0 0 auto;-moz-box-flex:0;-moz-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto}.header .container,.footer .container{display:-webkit-box;display:-webkit-flex;display:-moz-flex;display:-ms-flexbox;display:flex;-webkit-box-direction:normal;-webkit-box-orient:horizontal;-webkit-flex-direction:row;-moz-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;-webkit-justify-content:space-between;-moz-justify-content:space-between;justify-content:space-between;padding:1rem 0}@media screen and (min-width: 40em){.header .container,.footer .container{-webkit-box-align:center;-ms-flex-align:center;-webkit-align-items:center;-moz-align-items:center;align-items:center}}.header .nav{display:flex;flex-direction:column-reverse;align-items:flex-end;justify-content:flex-end;overflow:hidden}.feature{padding-bottom:.4rem;margin-bottom:1.6rem;text-align:center;background:#f9fafa;background-size:cover;background-position:center}.feature .container{min-height:35vh;max-width:60%;-webkit-box-direction:normal;-webkit-box-orient:vertical;-webkit-flex-direction:column;-moz-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;-moz-justify-content:center;justify-content:center}.logo{display:inline-block;line-height:0;max-height:4rem;display:-webkit-box;display:-webkit-flex;display:-moz-flex;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;-webkit-align-items:center;-moz-align-items:center;align-items:center}.logo img{max-height:5rem}.nav a{padding:.2rem}.nav--social a{display:inline-block;line-height:1}.small{padding-top:.8rem;color:#a8adac;display:inline-block}.footer{background:#242e2b;color:#a8adac}.footer a{color:#a8adac}.footer a:hover{color:#fff}.footer .container{-webkit-flex-wrap:wrap;-moz-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap}.footer .small{padding-top:0}.nav--paginator{display:-webkit-box;display:-webkit-flex;display:-moz-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-ms-flex-pack:justify;-webkit-justify-content:space-between;-moz-justify-content:space-between;justify-content:space-between;color:#a8adac;text-align:center}.pagination{min-width:20%}.main{display:-webkit-box;display:-webkit-flex;display:-moz-flex;display:-ms-flexbox;display:flex;-webkit-box-direction:normal;-webkit-box-orient:vertical;-webkit-flex-direction:column;-moz-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex:1 0 auto;-moz-box-flex:1;-moz-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;-moz-justify-content:flex-start;justify-content:flex-start;margin-bottom:1.6rem}@media screen and (min-width: 40em){.main{-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;-moz-justify-content:center;justify-content:center;-webkit-box-direction:normal;-webkit-box-orient:horizontal;-webkit-flex-direction:row;-moz-flex-direction:row;-ms-flex-direction:row;flex-direction:row}}.header,.feature,.footer{-webkit-box-flex:0;-webkit-flex:0 0 auto;-moz-box-flex:0;-moz-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto}@media screen and (min-width: 40em){.content{width:62%}}.share .button{margin-right:.3rem}@media screen and (min-width: 40em){.aside{max-width:34%;margin-left:4%;-webkit-box-flex:1;-webkit-flex:1 0 auto;-moz-box-flex:1;-moz-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;opacity:.5;transition:opacity .1s}.aside .section{position:sticky;top:0}.aside--left{margin-left:0;margin-right:4%;order:-1}.aside:hover{opacity:1}}.section{margin:0}.list{list-style:none}.list--nav{overflow:hidden;margin-bottom:var(--listHeight, 0)}.list .item--post,.list .item--result,.list .item--category{margin-left:0}@media screen and (min-width: 40em){.item--nav{margin-left:.4rem;display:inline-block}}.item--nav:first-of-type{margin-left:0}.item--current a{color:#a8adac}a .icon{transition:fill .1s}a .icon:hover{fill:currentColor}.icon{vertical-align:middle;width:1.1em;height:1.1em;fill:CurrentColor}img{max-width:100%;height:auto}.figure{line-height:0}.figure--left{float:left;padding-right:.8rem}.figure--right{float:right;text-align:right;padding-left:.8rem}.figure--center{text-align:center;clear:both}.video,.map{position:relative;padding-bottom:56.25%;height:0}.video iframe,.map iframe{position:absolute;top:0;left:0;width:100%;height:100%}.map{padding-bottom:70%}.figure,.video,.map,.form{margin-bottom:.8rem}audio,video{width:100%}.form{position:relative}.form__legend{font-style:italic;color:#a8adac;position:absolute;overflow:hidden;right:0;clip:rect(0 0 0 0)}button,.button,input[type=text],input[type=email],input[type=search],input[type=submit],input[type=color],textarea,select{padding:.6rem 1.2rem;margin-bottom:.6rem;transition:color .1s,background-color .1s,border .1s;line-height:inherit;border:none;box-shadow:none;border-radius:0;-webkit-appearance:none}button .icon,.button .icon,input[type=text] .icon,input[type=email] .icon,input[type=search] .icon,input[type=submit] .icon,input[type=color] .icon,textarea .icon,select .icon{margin:0 0 .35rem}input[type=submit],button,.button{cursor:pointer;display:inline-block;color:#fff;background:#05bf85;transition:box-shadow .1s;will-change:box-shadow;box-shadow:inset 0 0 0 2rem rgba(0,0,0,0)}input[type=submit]:hover,button:hover,.button:hover{box-shadow:inset 0 0 0 2rem rgba(0,0,0,.25)}input[type=submit]:active,input[type=submit]:focus,button:active,button:focus,.button:active,.button:focus{box-shadow:inset 0 0 0 2rem rgba(0,0,0,.25)}.button--nav{background:none;color:#05bf85;margin-bottom:0;min-height:4rem;min-width:4rem}@media screen and (min-width: 40em){.button--nav{display:none}}.button--nav:hover,.button--nav:focus{box-shadow:none;color:green}a:focus,a:hover:focus,button:focus,button:hover:focus,.button:focus,.button:hover:focus,input:focus,input:hover:focus,textarea:focus,textarea:hover:focus,select:focus,select:hover:focus{outline:solid .12rem #fa407a;outline-offset:-0.12rem}a:hover:focus{outline:none}input[type=text],input[type=email],input[type=search],input[type=color],textarea,select{width:100%;border:1px solid #a8adac}input[type=text]:hover,input[type=email]:hover,input[type=search]:hover,input[type=color]:hover,textarea:hover,select:hover{border-color:#384743}select{background:#fafafa}textarea{resize:vertical}label{line-height:2rem;margin-bottom:1.49999rem;padding-top:0.50001rem}@media screen and (min-width: 40em){label{margin-bottom:1.4553471429rem;padding-top:0.5446528571rem}}@media screen and (min-width: 50em){label{margin-bottom:1.44999rem;padding-top:0.55001rem}}@media screen and (min-width: 64em){label{margin-bottom:1.4411664706rem;padding-top:0.5588335294rem}}@media screen and (min-width: 100em){label{margin-bottom:1.4342005263rem;padding-top:0.5657994737rem}}code{padding:.12rem .2rem;color:#242e2b}pre code{padding:0}.required{color:red}::-webkit-input-placeholder{opacity:.5}::-moz-placeholder{opacity:.5}:-ms-input-placeholder{opacity:.5}:-moz-placeholder{opacity:.5}::selection{background:#242e2b;color:#fff;text-shadow:none}.typeset a>code{text-shadow:none}.typeset .button,.typeset button{background-image:none;text-shadow:none;color:#fff}.typeset .button:hover,.typeset .button:active,.typeset .button:focus,.typeset button:hover,.typeset button:active,.typeset button:focus{background-image:none;color:#fff}.typeset hr{width:100%}.typeset li>p{padding:0;margin:0}.typeset .nav a{padding-left:0;padding-right:0;margin-left:.2rem;margin-right:.2rem}.typeset pre{white-space:pre;overflow-x:scroll}</style>

    

    
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@16px.png" sizes="16x16">
    <link rel="apple-touch-icon" sizes="16x16" href="/assets/logos/logo@16px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@32px.png" sizes="32x32">
    <link rel="apple-touch-icon" sizes="32x32" href="/assets/logos/logo@32px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@96px.png" sizes="96x96">
    <link rel="apple-touch-icon" sizes="96x96" href="/assets/logos/logo@96px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@120px.png" sizes="120x120">
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/logos/logo@120px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@144px.png" sizes="144x144">
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/logos/logo@144px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@180px.png" sizes="180x180">
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/logos/logo@180px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@512px.png" sizes="512x512">
    <link rel="apple-touch-icon" sizes="512x512" href="/assets/logos/logo@512px.png">
  
    <link rel="icon" type="image/png" href="/assets/logos/logo@1024px.png" sizes="1024x1024">
    <link rel="apple-touch-icon" sizes="1024x1024" href="/assets/logos/logo@1024px.png">
  

<link rel="shortcut icon" href="/assets/logos/logo.svg">


    

    
    
        <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&display=swap" />
        <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&display=swap" rel="stylesheet" media="print" onload="this.media='all'" />
    
    <noscript>
        
            <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&display=swap" rel="stylesheet" />
        
    </noscript>



    <!-- Overwrite this file with code you want before the closing head tag -->

  </head>

  <body class="layout-post  traffic-signs-detection">
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="height: 0; position: absolute">
  <symbol id="codepen" viewBox="0 0 16 16"><path d="M15.988 5.443c-.004-.02-.007-.04-.012-.058l-.01-.033c-.006-.017-.012-.034-.02-.05-.003-.012-.01-.023-.014-.034l-.023-.045-.02-.032-.03-.04-.024-.03c-.01-.013-.022-.026-.034-.038l-.027-.027-.04-.032-.03-.024-.012-.01L8.38.117c-.23-.155-.53-.155-.76 0L.305 4.99.296 5c-.012.007-.022.015-.032.023-.014.01-.027.02-.04.032l-.027.027-.034.037-.024.03-.03.04c-.006.012-.013.022-.02.033l-.023.045-.015.034c-.007.016-.012.033-.018.05l-.01.032c-.005.02-.01.038-.012.058l-.006.03C.002 5.5 0 5.53 0 5.56v4.875c0 .03.002.06.006.09l.007.03c.003.02.006.04.013.058l.01.033c.006.018.01.035.018.05l.015.033c.006.016.014.03.023.047l.02.03c.008.016.018.03.03.042.007.01.014.02.023.03.01.012.02.025.034.036.01.01.018.02.028.026l.04.033.03.023.01.01 7.31 4.876c.116.078.248.117.382.116.134 0 .266-.04.38-.116l7.314-4.875.01-.01c.012-.007.022-.015.032-.023.014-.01.027-.02.04-.032l.027-.027.034-.037.024-.03.03-.04.02-.032.023-.046.015-.033.018-.052.01-.033c.005-.02.01-.038.013-.058 0-.01.003-.02.004-.03.004-.03.006-.06.006-.09V5.564c0-.03-.002-.06-.006-.09l-.007-.03zM8 9.626L5.568 8 8 6.374 10.432 8 8 9.626zM7.312 5.18l-2.98 1.993-2.406-1.61 5.386-3.59v3.206zM3.095 8l-1.72 1.15v-2.3L3.095 8zm1.237.828l2.98 1.993v3.208l-5.386-3.59 2.406-1.61zm4.355 1.993l2.98-1.993 2.407 1.61-5.387 3.59v-3.206zM12.905 8l1.72-1.15v2.3L12.905 8zm-1.237-.827L8.688 5.18V1.97l5.386 3.59-2.406 1.61z" fill-rule="nonzero"/></symbol>
  <symbol id="dribbble" viewBox="0 0 16 16"><path d="M8 16c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zm6.747-6.905c-.234-.074-2.115-.635-4.257-.292.894 2.456 1.258 4.456 1.328 4.872 1.533-1.037 2.624-2.68 2.93-4.58zM10.67 14.3c-.102-.6-.5-2.688-1.46-5.18l-.044.014C5.312 10.477 3.93 13.15 3.806 13.4c1.158.905 2.614 1.444 4.194 1.444.947 0 1.85-.194 2.67-.543zm-7.747-1.72c.155-.266 2.03-3.37 5.555-4.51.09-.03.18-.056.27-.08-.173-.39-.36-.778-.555-1.16C4.78 7.85 1.47 7.807 1.17 7.8l-.003.208c0 1.755.665 3.358 1.756 4.57zM1.31 6.61c.307.005 3.122.017 6.318-.832-1.132-2.012-2.353-3.705-2.533-3.952-1.912.902-3.34 2.664-3.784 4.785zM6.4 1.368c.188.253 1.43 1.943 2.548 4 2.43-.91 3.46-2.293 3.582-2.468C11.323 1.827 9.736 1.176 8 1.176c-.55 0-1.087.066-1.6.19zm6.89 2.322c-.145.194-1.29 1.662-3.816 2.694.16.325.31.656.453.99.05.117.1.235.147.352 2.274-.286 4.533.172 4.758.22-.015-1.613-.59-3.094-1.543-4.257z"/></symbol>
  <symbol id="designernews" viewBox="0 0 16 16"><path d="M7.514 7.988c0-2.555-1.57-4.287-4.56-4.287H0v8.6h3.016c2.903 0 4.498-1.75 4.498-4.31zM5.37 8c0 1.844-.946 2.642-2.467 2.642H2.13V5.358h.773C4.36 5.358 5.37 6.193 5.37 8zM16 12.3V3.7h-1.98v4.81L10.853 3.7h-2.07v8.6h1.982V7.152l3.39 5.146H16z"/></symbol>
  <symbol id="facebook" viewBox="0 0 16 16"><path d="M15.117 0H.883C.395 0 0 .395 0 .883v14.234c0 .488.395.883.883.883h7.663V9.804H6.46V7.39h2.086V5.607c0-2.066 1.262-3.19 3.106-3.19.883 0 1.642.064 1.863.094v2.16h-1.28c-1 0-1.195.476-1.195 1.176v1.54h2.39l-.31 2.416h-2.08V16h4.077c.488 0 .883-.395.883-.883V.883C16 .395 15.605 0 15.117 0" fill-rule="nonzero"/></symbol>
  <symbol id="flickr" viewBox="0 0 16 16"><path d="M0 8c0 2.05 1.662 3.71 3.71 3.71 2.05 0 3.713-1.66 3.713-3.71S5.76 4.29 3.71 4.29C1.663 4.29 0 5.95 0 8zm8.577 0c0 2.05 1.662 3.71 3.712 3.71C14.337 11.71 16 10.05 16 8s-1.662-3.71-3.71-3.71c-2.05 0-3.713 1.66-3.713 3.71z"/></symbol>
  <symbol id="github" viewBox="0 0 16 16"><path d="M8 0C3.58 0 0 3.582 0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385 0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953 0-.873.31-1.587.823-2.147-.083-.202-.358-1.015.077-2.117 0 0 .672-.215 2.2.82.638-.178 1.323-.266 2.003-.27.68.004 1.364.092 2.003.27 1.527-1.035 2.198-.82 2.198-.82.437 1.102.163 1.915.08 2.117.513.56.823 1.274.823 2.147 0 3.073-1.87 3.75-3.653 3.947.287.246.543.735.543 1.48 0 1.07-.01 1.933-.01 2.195 0 .215.144.463.55.385C13.71 14.53 16 11.534 16 8c0-4.418-3.582-8-8-8"/></symbol>
  <symbol id="hackernews" viewBox="0 0 16 16"><path d="M0 0v16h16V0H0zm8.92 8.96v3H7.25v-3l-2.75-5h1.96l1.66 3.48L9.7 3.96h1.88l-2.66 5z"/></symbol>
  <symbol id="instagram" viewBox="0 0 16 16"><path d="M8 0C5.827 0 5.555.01 4.702.048 3.85.088 3.27.222 2.76.42c-.526.204-.973.478-1.417.923-.445.444-.72.89-.923 1.417-.198.51-.333 1.09-.372 1.942C.008 5.555 0 5.827 0 8s.01 2.445.048 3.298c.04.852.174 1.433.372 1.942.204.526.478.973.923 1.417.444.445.89.72 1.417.923.51.198 1.09.333 1.942.372.853.04 1.125.048 3.298.048s2.445-.01 3.298-.048c.852-.04 1.433-.174 1.942-.372.526-.204.973-.478 1.417-.923.445-.444.72-.89.923-1.417.198-.51.333-1.09.372-1.942.04-.853.048-1.125.048-3.298s-.01-2.445-.048-3.298c-.04-.852-.174-1.433-.372-1.942-.204-.526-.478-.973-.923-1.417-.444-.445-.89-.72-1.417-.923-.51-.198-1.09-.333-1.942-.372C10.445.008 10.173 0 8 0zm0 1.44c2.136 0 2.39.01 3.233.048.78.036 1.203.166 1.485.276.374.145.64.318.92.598.28.28.453.546.598.92.11.282.24.705.276 1.485.038.844.047 1.097.047 3.233s-.01 2.39-.048 3.233c-.036.78-.166 1.203-.276 1.485-.145.374-.318.64-.598.92-.28.28-.546.453-.92.598-.282.11-.705.24-1.485.276-.844.038-1.097.047-3.233.047s-2.39-.01-3.233-.048c-.78-.036-1.203-.166-1.485-.276-.374-.145-.64-.318-.92-.598-.28-.28-.453-.546-.598-.92-.11-.282-.24-.705-.276-1.485C1.45 10.39 1.44 10.136 1.44 8s.01-2.39.048-3.233c.036-.78.166-1.203.276-1.485.145-.374.318-.64.598-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276C5.61 1.45 5.864 1.44 8 1.44zm0 2.452c-2.27 0-4.108 1.84-4.108 4.108 0 2.27 1.84 4.108 4.108 4.108 2.27 0 4.108-1.84 4.108-4.108 0-2.27-1.84-4.108-4.108-4.108zm0 6.775c-1.473 0-2.667-1.194-2.667-2.667 0-1.473 1.194-2.667 2.667-2.667 1.473 0 2.667 1.194 2.667 2.667 0 1.473-1.194 2.667-2.667 2.667zm5.23-6.937c0 .53-.43.96-.96.96s-.96-.43-.96-.96.43-.96.96-.96.96.43.96.96z"/></symbol>
  <symbol id="linkedin" viewBox="0 0 16 16"><path d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235 0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51V7.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4 0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762 0-1.376-.617-1.376-1.377 0-.758.614-1.375 1.376-1.375.76 0 1.376.617 1.376 1.375 0 .76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816 0H1.18C.528 0 0 .516 0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652 0 1.185-.516 1.185-1.153V1.153C16 .516 15.467 0 14.815 0z" fill-rule="nonzero"/></symbol>
  <symbol id="medium" viewBox="0 0 16 16"><path d="M11.824 12.628l-.276.45.798.398 2.744 1.372c.15.076.294.11.418.11.278 0 .467-.177.467-.492V5.883l-4.15 6.745zm4.096-8.67c-.004-.003 0-.01-.003-.012l-4.825-2.412c-.06-.03-.123-.038-.187-.044-.016 0-.03-.01-.047-.01-.184 0-.368.092-.467.254l-.24.39-.5.814-1.89 3.08 1.89 3.076.5.813.5.812.59.95 4.71-7.64c.02-.03.01-.06-.02-.08zm-6.27 7.045L7.17 6.97l-.295-.477-.294-.477-.25-.416v4.867l3.32 1.663.5.25.5.25-.5-.813-.5-.813zM.737 1.68L.59 1.608c-.085-.042-.166-.062-.24-.062-.206 0-.35.16-.35.427v10.162c0 .272.2.594.442.716l4.145 2.08c.107.06.208.08.3.08.257 0 .438-.2.438-.53V4.01c0-.02-.012-.04-.03-.047L.738 1.68z"/></symbol>
  <symbol id="pinterest" viewBox="0 0 16 16"><path d="M8 0C3.582 0 0 3.582 0 8c0 3.39 2.108 6.285 5.084 7.45-.07-.633-.133-1.604.028-2.295.146-.625.938-3.977.938-3.977s-.24-.48-.24-1.188c0-1.11.646-1.943 1.448-1.943.683 0 1.012.513 1.012 1.127 0 .687-.436 1.713-.662 2.664-.19.797.4 1.445 1.185 1.445 1.42 0 2.514-1.498 2.514-3.662 0-1.915-1.376-3.254-3.342-3.254-2.276 0-3.61 1.707-3.61 3.472 0 .687.263 1.424.593 1.825.066.08.075.15.057.23-.06.252-.196.796-.223.907-.035.146-.115.178-.268.107-.998-.465-1.624-1.926-1.624-3.1 0-2.524 1.834-4.84 5.287-4.84 2.774 0 4.932 1.977 4.932 4.62 0 2.757-1.74 4.977-4.153 4.977-.81 0-1.572-.422-1.833-.92l-.5 1.902c-.18.695-.667 1.566-.994 2.097.75.232 1.545.357 2.37.357 4.417 0 8-3.582 8-8s-3.583-8-8-8z" fill-rule="nonzero"/></symbol>
  <symbol id="rss" viewBox="0 0 16 16"><path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194 11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.985 2.194-2.196 2.194C.984 16 0 15.017 0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"/></symbol>
  <symbol id="reddit" viewBox="0 0 16 16"><path d="M1.473 9.368c-.04.185-.06.374-.06.566 0 2.3 2.94 4.173 6.554 4.173 3.613 0 6.553-1.872 6.553-4.173 0-.183-.02-.364-.055-.54l-.01-.022c-.013-.036-.02-.073-.02-.11-.2-.784-.745-1.497-1.533-2.072-.03-.01-.058-.026-.084-.047-.017-.013-.03-.028-.044-.043-1.198-.824-2.91-1.34-4.807-1.34-1.88 0-3.576.506-4.772 1.315-.01.012-.02.023-.033.033-.026.022-.056.04-.087.05-.805.576-1.364 1.293-1.572 2.086 0 .038-.01.077-.025.114l-.005.01zM8 13.003c-1.198 0-2.042-.26-2.58-.8-.116-.116-.116-.305 0-.422.117-.11.307-.11.424 0 .42.42 1.125.63 2.155.63 1.03 0 1.73-.2 2.15-.62.11-.11.3-.11.42 0 .11.12.11.31 0 .43-.54.54-1.38.8-2.58.8zM5.592 7.945c-.61 0-1.12.51-1.12 1.12 0 .608.51 1.102 1.12 1.102.61 0 1.103-.494 1.103-1.102 0-.61-.494-1.12-1.103-1.12zm4.83 0c-.61 0-1.12.51-1.12 1.12 0 .608.51 1.102 1.12 1.102.61 0 1.103-.494 1.103-1.102 0-.61-.494-1.12-1.103-1.12zM13.46 6.88c.693.556 1.202 1.216 1.462 1.94.3-.225.48-.578.48-.968 0-.67-.545-1.214-1.214-1.214-.267 0-.52.087-.728.243zM1.812 6.64c-.67 0-1.214.545-1.214 1.214 0 .363.16.7.43.927.268-.72.782-1.37 1.478-1.92-.202-.14-.443-.22-.694-.22zm6.155 8.067c-3.944 0-7.152-2.14-7.152-4.77 0-.183.016-.363.046-.54-.53-.33-.86-.91-.86-1.545 0-1 .82-1.812 1.82-1.812.45 0 .87.164 1.2.455 1.24-.796 2.91-1.297 4.75-1.33l1.21-3.69.27.063s.01 0 .01.002l2.82.663c.23-.533.76-.908 1.38-.908.82 0 1.49.67 1.49 1.492 0 .823-.67 1.492-1.49 1.492s-1.49-.67-1.49-1.49L9.4 2.18l-.98 2.99c1.77.07 3.37.57 4.57 1.35.33-.31.764-.48 1.225-.48 1 0 1.814.81 1.814 1.81 0 .66-.36 1.26-.92 1.58.02.17.04.33.04.5-.01 2.63-3.21 4.77-7.16 4.77zM13.43 1.893c-.494 0-.895.4-.895.894 0 .493.4.894.894.894.49 0 .89-.4.89-.89s-.4-.89-.9-.89z"/></symbol>
  <symbol id="skype" viewBox="0 0 16 16"><path d="M8.035 12.6c-2.685 0-3.885-1.322-3.885-2.313 0-.51.374-.865.89-.865 1.15 0 .85 1.653 2.995 1.653 1.096 0 1.703-.597 1.703-1.208 0-.368-.18-.775-.904-.954l-2.387-.597C4.524 7.833 4.175 6.79 4.175 5.812c0-2.034 1.91-2.798 3.704-2.798 1.65 0 3.6.916 3.6 2.136 0 .523-.452.827-.97.827-.98 0-.798-1.36-2.773-1.36-.98 0-1.523.444-1.523 1.08 0 .636.774.84 1.446.993l1.767.392c1.936.433 2.427 1.566 2.427 2.633 0 1.652-1.266 2.886-3.82 2.886m7.4-3.264l-.014.084-.028-.16c.015.024.028.05.042.076.082-.45.125-.912.125-1.373 0-1.023-.2-2.014-.595-2.948-.38-.902-.925-1.712-1.62-2.407-.692-.696-1.5-1.242-2.4-1.623C10.015.59 9.025.39 8.005.39c-.48 0-.963.045-1.43.135H6.57l.08.04-.16-.023.08-.016C5.927.183 5.205 0 4.472 0 3.278 0 2.155.466 1.31 1.313.465 2.16 0 3.286 0 4.483c0 .763.195 1.512.563 2.175l.013-.083.028.16c-.015-.026-.027-.052-.04-.077-.076.43-.115.867-.115 1.305 0 1.022.2 2.014.593 2.948.38.903.925 1.713 1.62 2.408.693.695 1.5 1.242 2.4 1.623.932.397 1.92.597 2.94.597.445 0 .89-.04 1.325-.118l-.077-.043.162.028-.084.014c.67.378 1.426.58 2.2.58 1.194 0 2.317-.466 3.162-1.313.845-.846 1.31-1.972 1.31-3.17 0-.765-.197-1.517-.566-2.18" fill-rule="nonzero"/></symbol>
  <symbol id="tumblr" viewBox="0 0 16 16"><path d="M9.708 16c-3.396 0-4.687-2.504-4.687-4.274V6.498H3.403V4.432C5.83 3.557 6.412 1.368 6.55.12c.01-.086.077-.12.115-.12H9.01v4.076h3.2v2.422H8.997v4.98c.01.667.25 1.58 1.472 1.58h.067c.424-.012.994-.136 1.29-.278l.77 2.283c-.288.424-1.594.916-2.77.936h-.12z" fill-rule="nonzero"/></symbol>
  <symbol id="twitch" viewBox="0 0 16 16"><g fill-rule="nonzero"><path d="M1.393 0L.35 2.783v11.13h3.824V16h2.088l2.085-2.088h3.13L15.65 9.74V0H1.394zm1.39 1.39H14.26v7.653l-2.435 2.435H8l-2.085 2.085v-2.085H2.783V1.39z"/><path d="M6.61 8.348H8V4.175H6.61v4.173zm3.824 0h1.39V4.175h-1.39v4.173z"/></g></symbol>
  <symbol id="twitter" viewBox="0 0 16 16"><path d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.375-1.337.648-2.085.795-.598-.638-1.45-1.036-2.396-1.036-1.812 0-3.282 1.468-3.282 3.28 0 .258.03.51.085.75C5.152 5.39 2.733 4.084 1.114 2.1.83 2.583.67 3.147.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21 0-.416-.02-.617-.058.418 1.304 1.63 2.253 3.067 2.28-1.124.88-2.54 1.404-4.077 1.404-.265 0-.526-.015-.783-.045 1.453.93 3.178 1.474 5.032 1.474 6.038 0 9.34-5 9.34-9.338 0-.143-.004-.284-.01-.425.64-.463 1.198-1.04 1.638-1.7z" fill-rule="nonzero"/></symbol>
  <symbol id="vimeo" viewBox="0 0 16 16"><path d="M15.992 4.275c-.07 1.56-1.16 3.697-3.263 6.41-2.176 2.832-4.017 4.248-5.522 4.248-.933 0-1.722-.862-2.367-2.588L3.55 7.6c-.48-1.724-.993-2.587-1.542-2.587-.12 0-.538.252-1.255.755L0 4.796C.79 4.1 1.568 3.406 2.335 2.71c1.053-.912 1.844-1.39 2.37-1.44 1.246-.12 2.012.733 2.3 2.56.31 1.97.526 3.194.647 3.673.36 1.634.754 2.45 1.185 2.45.335 0 .838-.53 1.51-1.59.67-1.06 1.028-1.866 1.076-2.42.096-.915-.263-1.374-1.077-1.374-.383 0-.778.087-1.185.262.788-2.58 2.29-3.834 4.508-3.762 1.644.048 2.42 1.116 2.324 3.205z" fill-rule="nonzero"/></symbol>
  <symbol id="youtube" viewBox="0 0 16 16"><path d="M0 7.345c0-1.294.16-2.59.16-2.59s.156-1.1.636-1.587c.608-.637 1.408-.617 1.764-.684C3.84 2.36 8 2.324 8 2.324s3.362.004 5.6.166c.314.038.996.04 1.604.678.48.486.636 1.588.636 1.588S16 6.05 16 7.346v1.258c0 1.296-.16 2.59-.16 2.59s-.156 1.102-.636 1.588c-.608.638-1.29.64-1.604.678-2.238.162-5.6.166-5.6.166s-4.16-.037-5.44-.16c-.356-.067-1.156-.047-1.764-.684-.48-.487-.636-1.587-.636-1.587S0 9.9 0 8.605v-1.26zm6.348 2.73V5.58l4.323 2.255-4.32 2.24h-.002z"/></symbol>
  <symbol id="link" viewBox="0 0 16 16"><path d="M5.86 12.7l-.81.8c-.7.7-1.84.7-2.54 0a1.75 1.75 0 0 1 0-2.5l2.98-2.96c.61-.61 1.77-1.52 2.62-.68a1 1 0 1 0 1.4-1.4c-1.44-1.43-3.57-1.17-5.42.67L1.1 9.6a3.72 3.72 0 0 0 0 5.32 3.78 3.78 0 0 0 5.34 0l.8-.8a1 1 0 1 0-1.39-1.42zm9.03-11.5c-1.55-1.53-3.7-1.6-5.14-.19l-1 1a1 1 0 1 0 1.39 1.41l1-1c.75-.74 1.72-.43 2.35.2a1.75 1.75 0 0 1 0 2.5l-3.17 3.15c-1.46 1.45-2.14.77-2.43.48a1 1 0 0 0-1.4 1.4c.67.67 1.43 1 2.23 1 .98 0 2.01-.5 3-1.47l3.17-3.15a3.72 3.72 0 0 0 0-5.32z"/></symbol>
  <symbol id="email" viewBox="0 0 16 11"><path fill-rule="evenodd" d="M1.33 0h13.34L8 5 1.33 0zM16 0v11H0V0l8 6 8-6z"/></symbol>
  <symbol id="nav" viewBox="0 0 16 11"><path d="M0 12h16v-2H0v2zm0-5h16V5H0v2zm0-7v2h16V0H0z"/></symbol>
</svg>


    <header class="header">
  <div class="container">
    <a class="logo" href="/">
  <img src="/assets/logos/logo.svg" alt="Alembic logo"/>
</a>


    
<nav class="nav  nav--header">
  <ul class="list  list--nav">
    

      

      <li class="item  item--nav">
        <a href="/">Home</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/projects/">Projects</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/blog/">Blog</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/resume/">Resume</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="/search/">Search</a>
      </li>
    

      

      <li class="item  item--nav">
        <a href="https://sonalithote.github.io/">About Me</a>
      </li>
    
  </ul>
  <button class="button  button--nav" aria-label="Menu toggle">
    <svg width="16" height="16" class="icon  icon--nav" role="img" alt="Menu"><title>Menu</title><use xlink:href="#nav" fill="CurrentColor"></use></svg>

  </button>
</nav>


<script type="text/javascript">
  // Get list and button
  const navList = document.querySelector('.header .list--nav')
  const button  = document.querySelector('.header .button--nav')

  // Hide nav and apply toggle
  const collapseNav = () => {
    if (document.body.clientWidth < 640) {
      navList.style.setProperty('--listHeight', `-${navList.offsetHeight}px`)
    } else {
      navList.removeAttribute('style')
    }

    button.onclick = () => {
      navList.style.setProperty('transition', `margin .1s`)
      if (navList.style.getPropertyValue('--listHeight')) {
        navList.style.removeProperty('--listHeight')
      } else {
        navList.style.setProperty('--listHeight', `-${navList.offsetHeight}px`)
      }
    }
  }

  collapseNav()

  // Check on resize if to collapse nav
  window.addEventListener('resize', () => {
    collapseNav()
  })
</script>

  </div>

  




  <div class="feature" style="background-image: url(/assets/signals.png)">
    <div class="container  typeset">
      <h2 id="detecting-traffic-signs">Detecting Traffic Signs</h2>
<p>Enhancing Autonomous Vehicle Technologies with Cutting-Edge Traffic Sign Detection Using YOLO v3, OpenCV, and Keras</p>

    </div>
  </div>



</header>


<main class="main  container">

  <article class="article  article--post  content  typeset">

    <h1>Traffic Signs Detection</h1>
    

<small class="small  post-meta">
  
  
    
      <span class="label  label--category"><a href="/categories/#ml-project">ML_project</a></span>
    
  &nbsp;&middot;&nbsp;<time datetime="2021-03-22T00:00:00+00:00" class="time">22 Mar 2021</time>
</small>

    <h1 id="traffic-signs-detection-with-opencv--keras">Traffic Signs Detection with OpenCV &amp; Keras</h1>

<p>Leveraging <strong>YOLO v3</strong>, <strong>OpenCV</strong>, and <strong>Keras</strong> for traffic signs detection offers a cutting-edge solution in computer vision, crucial for enhancing autonomous vehicle technologies. <strong>YOLO v3</strong> excels in real-time object detection, identifying traffic signs quickly and accurately. <strong>OpenCV</strong> aids in processing these images, while <strong>Keras</strong> specializes in classifying the detected signs into their respective categories, such as speed limits and stop signs. This combination delivers a powerful, efficient system for traffic sign detection, crucial for improving the safety and reliability of autonomous navigation systems.</p>

<ul>
  <li>Initially, a model trained within the Darknet framework utilizes the OpenCV dnn library to <strong>identify Traffic Signs across four distinct categories</strong>.</li>
  <li>Subsequently, another model, developed using Keras, <strong>classifies</strong> the segmented portions of these Traffic Signs into one of <strong>43 different classes</strong>.</li>
  <li>Although the results are currently experimental, they hold potential for future enhancements.</li>
</ul>

<p>Example of detections on video are shown below. <strong>Trained weights</strong> can be found in the course mentioned above.</p>

<p><img src="https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3400968%2Fbcdae0b57021d6ac3e86a9aa2e8c4b08%2Fts_detections.gif?generation=1581700736851192&amp;alt=media" alt="" /></p>

<h1 id="importing-required-libraries">Importing required libraries</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> <span class="c1"># linear algebra
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span> <span class="c1"># data processing, CSV file I/O (e.g. pd.read_csv)
</span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">timeit</span> <span class="kn">import</span> <span class="n">default_timer</span> <span class="k">as</span> <span class="n">timer</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>

<span class="c1"># Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="c1"># for dirname, _, filenames in os.walk('../input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))
</span>
<span class="k">print</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="s">'archive/'</span><span class="p">))</span>

<span class="c1"># Any results we write to the current directory are saved as output
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['test.pickle', 'data0.pickle', 'data2.pickle', 'std_gray.pickle', 'mean_image_rgb.pickle', 'data6.pickle', 'datasets_preparing.py', 'data8.pickle', 'data4.pickle', 'label_names.csv', 'data1.pickle', 'valid.pickle', 'std_rgb.pickle', 'data3.pickle', 'train.pickle', 'data7.pickle', 'mean_image_gray.pickle', 'data5.pickle', 'labels.pickle']
</code></pre></div></div>

<h1 id="-loading-labels"> Loading <em>labels</em></h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reading csv file with labels' names
# Loading two columns [0, 1] into Pandas dataFrame
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'../input/traffic-signs-preprocessed/label_names.csv'</span><span class="p">)</span>

<span class="c1"># Check point
# Showing first 5 rows from the dataFrame
</span><span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">.</span><span class="n">head</span><span class="p">())</span>
<span class="k">print</span><span class="p">()</span>

<span class="c1"># To locate by class number use one of the following
# ***.iloc[0][1] - returns element on the 0 column and 1 row
</span><span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Speed limit (20km/h)
# ***['SignName'][1] - returns element on the column with name 'SignName' and 1 row
</span><span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="s">'SignName'</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># Speed limit (30km/h)
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   ClassId              SignName
0        0  Speed limit (20km/h)
1        1  Speed limit (30km/h)
2        2  Speed limit (50km/h)
3        3  Speed limit (60km/h)
4        4  Speed limit (70km/h)

Speed limit (20km/h)
Speed limit (30km/h)
</code></pre></div></div>

<h1 id="-loading-trained-keras-cnn-model-for-classification"> Loading trained Keras CNN model for Classification</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes
</span><span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s">'../input/traffic-signs-classification-with-cnn/model-23x23.h5'</span><span class="p">)</span>

<span class="c1"># Loading mean image to use for preprocessing further
# Opening file for reading in binary mode
</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'../input/traffic-signs-preprocessed/mean_image_rgb.pickle'</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'latin1'</span><span class="p">)</span>  <span class="c1"># dictionary type
</span>    
<span class="k">print</span><span class="p">(</span><span class="n">mean</span><span class="p">[</span><span class="s">'mean_image_rgb'</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (3, 32, 32)
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(3, 32, 32)
</code></pre></div></div>

<h1 id="-loading-yolo-v3-network-by-opencv-dnn-library"> Loading YOLO v3 network by OpenCV dnn library</h1>

<h2 id="loading-trained-weights-and-cfg-file-into-the-network">Loading <em>trained weights</em> and <em>cfg file</em> into the Network</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Trained weights can be found in the course mentioned above
</span><span class="n">path_to_weights</span> <span class="o">=</span> <span class="s">'../input/trained-traffic-signs-detector-based-on-yolo-v3/yolov3_ts_train_5000.weights'</span>
<span class="n">path_to_cfg</span> <span class="o">=</span> <span class="s">'../input/traffic-signs-dataset-in-yolo-format/yolov3_ts_test.cfg'</span>

<span class="c1"># Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV
</span><span class="n">network</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">readNetFromDarknet</span><span class="p">(</span><span class="n">path_to_cfg</span><span class="p">,</span> <span class="n">path_to_weights</span><span class="p">)</span>

<span class="c1"># To use with GPU
</span><span class="n">network</span><span class="p">.</span><span class="n">setPreferableBackend</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">DNN_BACKEND_OPENCV</span><span class="p">)</span>
<span class="n">network</span><span class="p">.</span><span class="n">setPreferableTarget</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">DNN_TARGET_OPENCL_FP16</span><span class="p">)</span>

</code></pre></div></div>

<h2 id="getting-output-layers-where-detections-are-made">Getting <em>output layers</em> where detections are made</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Getting names of all YOLO v3 layers
</span><span class="n">layers_all</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">getLayerNames</span><span class="p">()</span>

<span class="c1"># Check point
# print(layers_all)
</span>
<span class="c1"># Getting only detection YOLO v3 layers that are 82, 94 and 106
</span><span class="n">layers_names_output</span> <span class="o">=</span> <span class="p">[</span><span class="n">layers_all</span><span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">network</span><span class="p">.</span><span class="n">getUnconnectedOutLayers</span><span class="p">()]</span>

<span class="c1"># Check point
</span><span class="k">print</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">layers_names_output</span><span class="p">)</span>  <span class="c1"># ['yolo_82', 'yolo_94', 'yolo_106']
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['yolo_82', 'yolo_94', 'yolo_106']
</code></pre></div></div>

<h2 id="setting-probability-threshold-and-colour-for-bounding-boxes">Setting <em>probability</em>, <em>threshold</em> and <em>colour</em> for bounding boxes</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Minimum probability to eliminate weak detections
</span><span class="n">probability_minimum</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># Setting threshold to filtering weak bounding boxes by non-maximum suppression
</span><span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># Generating colours for bounding boxes
# randint(low, high=None, size=None, dtype='l')
</span><span class="n">colours</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'uint8'</span><span class="p">)</span>

<span class="c1"># Check point
</span><span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">colours</span><span class="p">))</span>  <span class="c1"># &lt;class 'numpy.ndarray'&gt;
</span><span class="k">print</span><span class="p">(</span><span class="n">colours</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (43, 3)
</span><span class="k">print</span><span class="p">(</span><span class="n">colours</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># [25  65 200]
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'numpy.ndarray'&gt;
(43, 3)
[203  49  61]
</code></pre></div></div>

<h1 id="-reading-input-video"> Reading input video</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reading video from a file by VideoCapture object
# video = cv2.VideoCapture('../input/traffic-signs-dataset-in-yolo-format/traffic-sign-to-test.mp4')
# video = cv2.VideoCapture('../input/videofortesting/ts_video_1.mp4')
</span><span class="n">video</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="s">'../input/videofortesting/ts_video_6.mp4'</span><span class="p">)</span>

<span class="c1"># Writer that will be used to write processed frames
</span><span class="n">writer</span> <span class="o">=</span> <span class="bp">None</span>

<span class="c1"># Variables for spatial dimensions of the frames
</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>

</code></pre></div></div>

<h1 id="-processing-frames-in-the-loop"> Processing frames in the loop</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># Setting default size of plots
</span><span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Variable for counting total amount of frames
</span><span class="n">f</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Variable for counting total processing time
</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Catching frames in the loop
</span><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="c1"># Capturing frames one-by-one
</span>    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">video</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>

    <span class="c1"># If the frame was not retrieved
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
        <span class="k">break</span>
       
    <span class="c1"># Getting spatial dimensions of the frame for the first time
</span>    <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">h</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># Slicing two elements from tuple
</span>        <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">frame</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>

    <span class="c1"># Blob from current frame
</span>    <span class="n">blob</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">blobFromImage</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="p">(</span><span class="mi">416</span><span class="p">,</span> <span class="mi">416</span><span class="p">),</span> <span class="n">swapRB</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">crop</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="c1"># Forward pass with blob through output layers
</span>    <span class="n">network</span><span class="p">.</span><span class="n">setInput</span><span class="p">(</span><span class="n">blob</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">output_from_network</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">layers_names_output</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

    <span class="c1"># Increasing counters
</span>    <span class="n">f</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">t</span> <span class="o">+=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>

    <span class="c1"># Spent time for current frame
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'Frame number {0} took {1:.5f} seconds'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>

    <span class="c1"># Lists for detected bounding boxes, confidences and class's number
</span>    <span class="n">bounding_boxes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">confidences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">class_numbers</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Going through all output layers after feed forward pass
</span>    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">output_from_network</span><span class="p">:</span>
        <span class="c1"># Going through all detections from current output layer
</span>        <span class="k">for</span> <span class="n">detected_objects</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
            <span class="c1"># Getting 80 classes' probabilities for current detected object
</span>            <span class="n">scores</span> <span class="o">=</span> <span class="n">detected_objects</span><span class="p">[</span><span class="mi">5</span><span class="p">:]</span>
            <span class="c1"># Getting index of the class with the maximum value of probability
</span>            <span class="n">class_current</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
            <span class="c1"># Getting value of probability for defined class
</span>            <span class="n">confidence_current</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">class_current</span><span class="p">]</span>

            <span class="c1"># Eliminating weak predictions by minimum probability
</span>            <span class="k">if</span> <span class="n">confidence_current</span> <span class="o">&gt;</span> <span class="n">probability_minimum</span><span class="p">:</span>
                <span class="c1"># Scaling bounding box coordinates to the initial frame size
</span>                <span class="n">box_current</span> <span class="o">=</span> <span class="n">detected_objects</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>

                <span class="c1"># Getting top left corner coordinates
</span>                <span class="n">x_center</span><span class="p">,</span> <span class="n">y_center</span><span class="p">,</span> <span class="n">box_width</span><span class="p">,</span> <span class="n">box_height</span> <span class="o">=</span> <span class="n">box_current</span>
                <span class="n">x_min</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">x_center</span> <span class="o">-</span> <span class="p">(</span><span class="n">box_width</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
                <span class="n">y_min</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">y_center</span> <span class="o">-</span> <span class="p">(</span><span class="n">box_height</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>

                <span class="c1"># Adding results into prepared lists
</span>                <span class="n">bounding_boxes</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">box_width</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">box_height</span><span class="p">)])</span>
                <span class="n">confidences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">confidence_current</span><span class="p">))</span>
                <span class="n">class_numbers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">class_current</span><span class="p">)</span>
                

    <span class="c1"># Implementing non-maximum suppression of given bounding boxes
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">NMSBoxes</span><span class="p">(</span><span class="n">bounding_boxes</span><span class="p">,</span> <span class="n">confidences</span><span class="p">,</span> <span class="n">probability_minimum</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>

    <span class="c1"># Checking if there is any detected object been left
</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Going through indexes of results
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="n">flatten</span><span class="p">():</span>
            <span class="c1"># Bounding box coordinates, its width and height
</span>            <span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span> <span class="o">=</span> <span class="n">bounding_boxes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">bounding_boxes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">box_width</span><span class="p">,</span> <span class="n">box_height</span> <span class="o">=</span> <span class="n">bounding_boxes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">bounding_boxes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span>
            
            
            <span class="c1"># Cut fragment with Traffic Sign
</span>            <span class="n">c_ts</span> <span class="o">=</span> <span class="n">frame</span><span class="p">[</span><span class="n">y_min</span><span class="p">:</span><span class="n">y_min</span><span class="o">+</span><span class="nb">int</span><span class="p">(</span><span class="n">box_height</span><span class="p">),</span> <span class="n">x_min</span><span class="p">:</span><span class="n">x_min</span><span class="o">+</span><span class="nb">int</span><span class="p">(</span><span class="n">box_width</span><span class="p">),</span> <span class="p">:]</span>
            <span class="c1"># print(c_ts.shape)
</span>            
            <span class="k">if</span> <span class="n">c_ts</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="ow">or</span> <span class="n">c_ts</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="mi">0</span><span class="p">,):</span>
                <span class="k">pass</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Getting preprocessed blob with Traffic Sign of needed shape
</span>                <span class="n">blob_ts</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">blobFromImage</span><span class="p">(</span><span class="n">c_ts</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">swapRB</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">crop</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
                <span class="n">blob_ts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">blob_ts</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">mean</span><span class="p">[</span><span class="s">'mean_image_rgb'</span><span class="p">]</span>
                <span class="n">blob_ts</span> <span class="o">=</span> <span class="n">blob_ts</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="c1"># plt.imshow(blob_ts[0, :, :, :])
</span>                <span class="c1"># plt.show()
</span>
                <span class="c1"># Feeding to the Keras CNN model to get predicted label among 43 classes
</span>                <span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">blob_ts</span><span class="p">)</span>

                <span class="c1"># Scores is given for image with 43 numbers of predictions for each class
</span>                <span class="c1"># Getting only one class with maximum value
</span>                <span class="n">prediction</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
                <span class="c1"># print(labels['SignName'][prediction])
</span>

                <span class="c1"># Colour for current bounding box
</span>                <span class="n">colour_box_current</span> <span class="o">=</span> <span class="n">colours</span><span class="p">[</span><span class="n">class_numbers</span><span class="p">[</span><span class="n">i</span><span class="p">]].</span><span class="n">tolist</span><span class="p">()</span>

                <span class="c1"># Drawing bounding box on the original current frame
</span>                <span class="n">cv2</span><span class="p">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="p">),</span>
                              <span class="p">(</span><span class="n">x_min</span> <span class="o">+</span> <span class="n">box_width</span><span class="p">,</span> <span class="n">y_min</span> <span class="o">+</span> <span class="n">box_height</span><span class="p">),</span>
                              <span class="n">colour_box_current</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

                <span class="c1"># Preparing text with label and confidence for current bounding box
</span>                <span class="n">text_box_current</span> <span class="o">=</span> <span class="s">'{}: {:.4f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="s">'SignName'</span><span class="p">][</span><span class="n">prediction</span><span class="p">],</span>
                                                       <span class="n">confidences</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

                <span class="c1"># Putting text with label and confidence on the original image
</span>                <span class="n">cv2</span><span class="p">.</span><span class="n">putText</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">text_box_current</span><span class="p">,</span> <span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span> <span class="o">-</span> <span class="mi">5</span><span class="p">),</span>
                            <span class="n">cv2</span><span class="p">.</span><span class="n">FONT_HERSHEY_SIMPLEX</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">colour_box_current</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>


    <span class="c1"># Initializing writer only once
</span>    <span class="k">if</span> <span class="n">writer</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">fourcc</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">VideoWriter_fourcc</span><span class="p">(</span><span class="o">*</span><span class="s">'mp4v'</span><span class="p">)</span>

        <span class="c1"># Writing current processed frame into the video file
</span>        <span class="n">writer</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">VideoWriter</span><span class="p">(</span><span class="s">'result.mp4'</span><span class="p">,</span> <span class="n">fourcc</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span>
                                 <span class="p">(</span><span class="n">frame</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">frame</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Write processed current frame to the file
</span>    <span class="n">writer</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>


<span class="c1"># Releasing video reader and writer
</span><span class="n">video</span><span class="p">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">writer</span><span class="p">.</span><span class="n">release</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Frame number 1 took 5.52782 seconds
Frame number 2 took 0.30848 seconds
Frame number 3 took 0.28469 seconds
Frame number 4 took 0.28405 seconds
Frame number 5 took 0.28431 seconds
Frame number 6 took 0.28376 seconds
Frame number 7 took 0.28409 seconds
Frame number 8 took 0.28311 seconds
Frame number 9 took 0.28223 seconds
Frame number 10 took 0.28190 seconds
Frame number 11 took 0.28196 seconds
Frame number 12 took 0.28338 seconds
Frame number 13 took 0.28229 seconds
Frame number 14 took 0.28356 seconds
Frame number 15 took 0.28600 seconds
Frame number 16 took 0.29676 seconds
Frame number 17 took 0.28499 seconds
Frame number 18 took 0.28229 seconds
Frame number 19 took 0.28268 seconds
Frame number 20 took 0.28224 seconds
Frame number 21 took 0.28413 seconds
Frame number 22 took 0.28356 seconds
Frame number 23 took 0.28463 seconds
Frame number 24 took 0.28319 seconds
Frame number 25 took 0.28683 seconds
Frame number 26 took 0.28725 seconds
Frame number 27 took 0.28898 seconds
Frame number 28 took 0.28659 seconds
Frame number 29 took 0.28389 seconds
Frame number 30 took 0.28378 seconds
Frame number 31 took 0.28475 seconds
Frame number 32 took 0.28251 seconds
Frame number 33 took 0.28269 seconds
Frame number 34 took 0.28348 seconds
Frame number 35 took 0.28382 seconds
Frame number 36 took 0.28517 seconds
Frame number 37 took 0.28654 seconds
Frame number 38 took 0.28655 seconds
Frame number 39 took 0.28563 seconds
Frame number 40 took 0.28403 seconds
Frame number 41 took 0.28611 seconds
Frame number 42 took 0.28182 seconds
Frame number 43 took 0.29107 seconds
Frame number 44 took 0.28453 seconds
Frame number 45 took 0.28377 seconds
Frame number 46 took 0.28447 seconds
Frame number 47 took 0.28513 seconds
Frame number 48 took 0.28379 seconds
Frame number 49 took 0.28776 seconds
Frame number 50 took 0.28735 seconds
Frame number 51 took 0.28501 seconds
Frame number 52 took 0.28487 seconds
Frame number 53 took 0.28414 seconds
Frame number 54 took 0.28307 seconds
Frame number 55 took 0.28253 seconds
Frame number 56 took 0.28466 seconds
Frame number 57 took 0.28470 seconds
Frame number 58 took 0.28420 seconds
Frame number 59 took 0.28372 seconds
Frame number 60 took 0.28218 seconds
Frame number 61 took 0.28171 seconds
Frame number 62 took 0.28202 seconds
Frame number 63 took 0.28421 seconds
Frame number 64 took 0.28256 seconds
Frame number 65 took 0.28365 seconds
Frame number 66 took 0.28422 seconds
Frame number 67 took 0.28526 seconds
Frame number 68 took 0.28542 seconds
Frame number 69 took 0.28809 seconds
Frame number 70 took 0.30996 seconds
Frame number 71 took 0.39311 seconds
Frame number 72 took 0.29010 seconds
Frame number 73 took 0.28246 seconds
Frame number 74 took 0.28550 seconds
Frame number 75 took 0.28487 seconds
Frame number 76 took 0.28557 seconds
Frame number 77 took 0.28503 seconds
Frame number 78 took 0.28439 seconds
Frame number 79 took 0.28619 seconds
Frame number 80 took 0.28490 seconds
Frame number 81 took 0.28697 seconds
Frame number 82 took 0.28585 seconds
Frame number 83 took 0.28697 seconds
Frame number 84 took 0.28652 seconds
Frame number 85 took 0.28533 seconds
Frame number 86 took 0.28449 seconds
Frame number 87 took 0.28467 seconds
Frame number 88 took 0.28317 seconds
Frame number 89 took 0.28570 seconds
Frame number 90 took 0.28561 seconds
Frame number 91 took 0.28517 seconds
Frame number 92 took 0.28556 seconds
Frame number 93 took 0.28932 seconds
Frame number 94 took 0.28604 seconds
Frame number 95 took 0.28409 seconds
Frame number 96 took 0.29276 seconds
Frame number 97 took 0.28508 seconds
Frame number 98 took 0.28641 seconds
Frame number 99 took 0.28600 seconds
Frame number 100 took 0.28778 seconds
Frame number 101 took 0.28482 seconds
Frame number 102 took 0.28302 seconds
Frame number 103 took 0.28462 seconds
Frame number 104 took 0.28550 seconds
Frame number 105 took 0.28799 seconds
Frame number 106 took 0.28656 seconds
Frame number 107 took 0.28632 seconds
Frame number 108 took 0.28320 seconds
Frame number 109 took 0.28345 seconds
Frame number 110 took 0.28199 seconds
Frame number 111 took 0.28399 seconds
Frame number 112 took 0.28442 seconds
Frame number 113 took 0.28563 seconds
Frame number 114 took 0.28655 seconds
Frame number 115 took 0.28611 seconds
Frame number 116 took 0.28686 seconds
Frame number 117 took 0.28606 seconds
Frame number 118 took 0.28554 seconds
Frame number 119 took 0.28580 seconds
Frame number 120 took 0.28680 seconds
Frame number 121 took 0.28843 seconds
Frame number 122 took 0.29228 seconds
Frame number 123 took 0.29320 seconds
Frame number 124 took 0.28505 seconds
Frame number 125 took 0.28440 seconds
Frame number 126 took 0.28295 seconds
Frame number 127 took 0.28477 seconds
Frame number 128 took 0.28561 seconds
Frame number 129 took 0.28503 seconds
Frame number 130 took 0.28481 seconds
Frame number 131 took 0.28668 seconds
Frame number 132 took 0.28682 seconds
Frame number 133 took 0.28491 seconds
Frame number 134 took 0.28558 seconds
Frame number 135 took 0.28669 seconds
Frame number 136 took 0.28582 seconds
Frame number 137 took 0.28552 seconds
Frame number 138 took 0.28360 seconds
Frame number 139 took 0.28467 seconds
Frame number 140 took 0.28590 seconds
Frame number 141 took 0.28458 seconds
Frame number 142 took 0.28605 seconds
Frame number 143 took 0.28407 seconds
Frame number 144 took 0.28660 seconds
Frame number 145 took 0.28519 seconds
Frame number 146 took 0.28497 seconds
Frame number 147 took 0.28567 seconds
Frame number 148 took 0.28722 seconds
Frame number 149 took 0.30203 seconds
Frame number 150 took 0.39440 seconds
Frame number 151 took 0.29713 seconds
Frame number 152 took 0.28998 seconds
Frame number 153 took 0.28315 seconds
Frame number 154 took 0.28381 seconds
Frame number 155 took 0.28484 seconds
Frame number 156 took 0.28455 seconds
Frame number 157 took 0.28462 seconds
Frame number 158 took 0.28355 seconds
Frame number 159 took 0.28561 seconds
Frame number 160 took 0.28295 seconds
Frame number 161 took 0.28152 seconds
Frame number 162 took 0.28430 seconds
Frame number 163 took 0.28459 seconds
Frame number 164 took 0.28397 seconds
Frame number 165 took 0.28378 seconds
Frame number 166 took 0.28418 seconds
Frame number 167 took 0.28647 seconds
Frame number 168 took 0.28554 seconds
Frame number 169 took 0.28408 seconds
Frame number 170 took 0.28628 seconds
Frame number 171 took 0.28269 seconds
Frame number 172 took 0.28406 seconds
Frame number 173 took 0.28388 seconds
Frame number 174 took 0.28510 seconds
Frame number 175 took 0.28671 seconds
Frame number 176 took 0.29481 seconds
Frame number 177 took 0.28572 seconds
Frame number 178 took 0.28461 seconds
Frame number 179 took 0.28354 seconds
Frame number 180 took 0.28381 seconds
Frame number 181 took 0.28487 seconds
Frame number 182 took 0.28755 seconds
Frame number 183 took 0.28590 seconds
Frame number 184 took 0.28326 seconds
Frame number 185 took 0.28332 seconds
Frame number 186 took 0.28574 seconds
Frame number 187 took 0.28746 seconds
Frame number 188 took 0.28684 seconds
Frame number 189 took 0.28440 seconds
Frame number 190 took 0.28410 seconds
Frame number 191 took 0.28283 seconds
Frame number 192 took 0.28129 seconds
Frame number 193 took 0.28304 seconds
Frame number 194 took 0.28127 seconds
Frame number 195 took 0.28551 seconds
Frame number 196 took 0.28479 seconds
Frame number 197 took 0.28270 seconds
Frame number 198 took 0.28455 seconds
Frame number 199 took 0.28374 seconds
Frame number 200 took 0.28207 seconds
Frame number 201 took 0.28154 seconds
Frame number 202 took 0.28349 seconds
Frame number 203 took 0.29308 seconds
Frame number 204 took 0.28364 seconds
Frame number 205 took 0.28461 seconds
Frame number 206 took 0.28661 seconds
Frame number 207 took 0.28589 seconds
Frame number 208 took 0.28265 seconds
Frame number 209 took 0.28450 seconds
Frame number 210 took 0.28276 seconds
Frame number 211 took 0.28015 seconds
Frame number 212 took 0.28183 seconds
Frame number 213 took 0.28343 seconds
Frame number 214 took 0.28340 seconds
Frame number 215 took 0.28382 seconds
Frame number 216 took 0.28503 seconds
Frame number 217 took 0.28606 seconds
Frame number 218 took 0.28351 seconds
Frame number 219 took 0.28184 seconds
Frame number 220 took 0.28242 seconds
Frame number 221 took 0.28519 seconds
Frame number 222 took 0.28396 seconds
Frame number 223 took 0.29134 seconds
Frame number 224 took 0.28385 seconds
Frame number 225 took 0.28655 seconds
Frame number 226 took 0.28653 seconds
Frame number 227 took 0.28642 seconds
Frame number 228 took 0.29651 seconds
Frame number 229 took 0.29838 seconds
Frame number 230 took 0.38873 seconds
Frame number 231 took 0.28324 seconds
Frame number 232 took 0.28234 seconds
Frame number 233 took 0.28309 seconds
Frame number 234 took 0.28333 seconds
Frame number 235 took 0.28514 seconds
Frame number 236 took 0.28152 seconds
Frame number 237 took 0.28373 seconds
Frame number 238 took 0.28316 seconds
Frame number 239 took 0.28187 seconds
Frame number 240 took 0.28413 seconds
Frame number 241 took 0.28420 seconds
Frame number 242 took 0.28467 seconds
Frame number 243 took 0.28419 seconds
Frame number 244 took 0.28625 seconds
Frame number 245 took 0.28635 seconds
Frame number 246 took 0.28186 seconds
Frame number 247 took 0.28371 seconds
Frame number 248 took 0.28258 seconds
Frame number 249 took 0.28080 seconds
Frame number 250 took 0.28274 seconds
Frame number 251 took 0.28412 seconds
Frame number 252 took 0.28426 seconds
Frame number 253 took 0.28682 seconds
Frame number 254 took 0.28167 seconds
Frame number 255 took 0.28321 seconds
Frame number 256 took 0.29472 seconds
Frame number 257 took 0.28530 seconds
Frame number 258 took 0.28456 seconds
Frame number 259 took 0.28545 seconds
Frame number 260 took 0.28461 seconds
Frame number 261 took 0.28277 seconds
Frame number 262 took 0.28404 seconds
Frame number 263 took 0.28606 seconds
Frame number 264 took 0.28602 seconds
Frame number 265 took 0.28313 seconds
Frame number 266 took 0.28395 seconds
Frame number 267 took 0.28183 seconds
Frame number 268 took 0.28174 seconds
Frame number 269 took 0.28410 seconds
Frame number 270 took 0.28458 seconds
Frame number 271 took 0.28675 seconds
Frame number 272 took 0.28688 seconds
Frame number 273 took 0.28503 seconds
Frame number 274 took 0.28500 seconds
Frame number 275 took 0.28310 seconds
Frame number 276 took 0.28119 seconds
Frame number 277 took 0.28336 seconds
Frame number 278 took 0.28553 seconds
Frame number 279 took 0.28352 seconds
Frame number 280 took 0.28545 seconds
Frame number 281 took 0.28195 seconds
Frame number 282 took 0.28735 seconds
Frame number 283 took 0.29508 seconds
Frame number 284 took 0.28249 seconds
Frame number 285 took 0.28274 seconds
Frame number 286 took 0.28265 seconds
Frame number 287 took 0.28077 seconds
Frame number 288 took 0.28335 seconds
Frame number 289 took 0.28146 seconds
Frame number 290 took 0.28254 seconds
Frame number 291 took 0.28325 seconds
Frame number 292 took 0.28674 seconds
Frame number 293 took 0.28403 seconds
Frame number 294 took 0.28442 seconds
Frame number 295 took 0.28130 seconds
Frame number 296 took 0.28412 seconds
Frame number 297 took 0.28505 seconds
Frame number 298 took 0.28399 seconds
Frame number 299 took 0.28320 seconds
Frame number 300 took 0.28322 seconds
Frame number 301 took 0.28711 seconds
Frame number 302 took 0.28616 seconds
Frame number 303 took 0.28305 seconds
Frame number 304 took 0.28422 seconds
Frame number 305 took 0.28305 seconds
Frame number 306 took 0.28035 seconds
Frame number 307 took 0.29199 seconds
Frame number 308 took 0.28817 seconds
Frame number 309 took 0.29844 seconds
Frame number 310 took 0.29078 seconds
Frame number 311 took 0.28241 seconds
Frame number 312 took 0.28323 seconds
Frame number 313 took 0.28173 seconds
Frame number 314 took 0.28147 seconds
Frame number 315 took 0.28425 seconds
Frame number 316 took 0.28448 seconds
Frame number 317 took 0.28600 seconds
Frame number 318 took 0.28327 seconds
Frame number 319 took 0.28452 seconds
Frame number 320 took 0.28637 seconds
Frame number 321 took 0.28627 seconds
Frame number 322 took 0.28629 seconds
Frame number 323 took 0.28339 seconds
Frame number 324 took 0.28367 seconds
Frame number 325 took 0.28587 seconds
Frame number 326 took 0.28509 seconds
Frame number 327 took 0.28538 seconds
Frame number 328 took 0.28441 seconds
Frame number 329 took 0.28467 seconds
Frame number 330 took 0.28634 seconds
Frame number 331 took 0.28423 seconds
Frame number 332 took 0.28498 seconds
Frame number 333 took 0.28668 seconds
Frame number 334 took 0.28574 seconds
Frame number 335 took 0.28656 seconds
Frame number 336 took 0.29757 seconds
Frame number 337 took 0.28516 seconds
Frame number 338 took 0.28614 seconds
Frame number 339 took 0.28451 seconds
Frame number 340 took 0.28847 seconds
Frame number 341 took 0.28675 seconds
Frame number 342 took 0.28578 seconds
Frame number 343 took 0.28292 seconds
Frame number 344 took 0.28233 seconds
Frame number 345 took 0.28436 seconds
Frame number 346 took 0.28223 seconds
Frame number 347 took 0.28319 seconds
Frame number 348 took 0.28307 seconds
Frame number 349 took 0.28340 seconds
Frame number 350 took 0.28312 seconds
Frame number 351 took 0.28410 seconds
Frame number 352 took 0.28347 seconds
Frame number 353 took 0.28253 seconds
Frame number 354 took 0.28212 seconds
Frame number 355 took 0.28306 seconds
Frame number 356 took 0.28282 seconds
Frame number 357 took 0.28295 seconds
Frame number 358 took 0.28414 seconds
Frame number 359 took 0.28654 seconds
Frame number 360 took 0.28786 seconds
Frame number 361 took 0.28524 seconds
Frame number 362 took 0.29588 seconds
Frame number 363 took 0.29353 seconds
Frame number 364 took 0.28240 seconds
Frame number 365 took 0.28102 seconds
Frame number 366 took 0.28119 seconds
Frame number 367 took 0.28203 seconds
Frame number 368 took 0.28123 seconds
Frame number 369 took 0.28110 seconds
Frame number 370 took 0.28499 seconds
Frame number 371 took 0.28321 seconds
Frame number 372 took 0.28100 seconds
Frame number 373 took 0.28231 seconds
Frame number 374 took 0.28319 seconds
Frame number 375 took 0.28479 seconds
Frame number 376 took 0.28427 seconds
Frame number 377 took 0.28608 seconds
Frame number 378 took 0.28534 seconds
Frame number 379 took 0.28462 seconds
Frame number 380 took 0.28592 seconds
Frame number 381 took 0.28679 seconds
Frame number 382 took 0.28434 seconds
Frame number 383 took 0.28003 seconds
Frame number 384 took 0.28291 seconds
Frame number 385 took 0.31018 seconds
Frame number 386 took 0.29572 seconds
Frame number 387 took 0.29071 seconds
Frame number 388 took 0.28215 seconds
Frame number 389 took 0.29660 seconds
Frame number 390 took 0.29125 seconds
Frame number 391 took 0.28239 seconds
Frame number 392 took 0.28199 seconds
Frame number 393 took 0.28238 seconds
Frame number 394 took 0.28447 seconds
Frame number 395 took 0.28208 seconds
Frame number 396 took 0.28237 seconds
Frame number 397 took 0.28580 seconds
Frame number 398 took 0.28642 seconds
Frame number 399 took 0.28645 seconds
Frame number 400 took 0.28367 seconds
Frame number 401 took 0.28445 seconds
Frame number 402 took 0.28423 seconds
Frame number 403 took 0.28436 seconds
Frame number 404 took 0.28554 seconds
Frame number 405 took 0.28653 seconds
Frame number 406 took 0.28459 seconds
Frame number 407 took 0.28579 seconds
Frame number 408 took 0.28109 seconds
Frame number 409 took 0.28283 seconds
Frame number 410 took 0.28278 seconds
Frame number 411 took 0.28106 seconds
Frame number 412 took 0.28101 seconds
Frame number 413 took 0.28220 seconds
Frame number 414 took 0.28197 seconds
Frame number 415 took 0.28409 seconds
Frame number 416 took 0.28440 seconds
Frame number 417 took 0.29191 seconds
Frame number 418 took 0.28376 seconds
Frame number 419 took 0.28163 seconds
Frame number 420 took 0.28463 seconds
Frame number 421 took 0.28094 seconds
Frame number 422 took 0.28145 seconds
Frame number 423 took 0.28248 seconds
Frame number 424 took 0.28267 seconds
Frame number 425 took 0.28153 seconds
Frame number 426 took 0.28141 seconds
Frame number 427 took 0.28267 seconds
Frame number 428 took 0.28160 seconds
Frame number 429 took 0.28295 seconds
Frame number 430 took 0.28292 seconds
Frame number 431 took 0.28278 seconds
Frame number 432 took 0.28456 seconds
Frame number 433 took 0.28434 seconds
Frame number 434 took 0.28545 seconds
Frame number 435 took 0.28577 seconds
Frame number 436 took 0.28626 seconds
Frame number 437 took 0.28582 seconds
Frame number 438 took 0.28445 seconds
Frame number 439 took 0.28572 seconds
Frame number 440 took 0.28439 seconds
Frame number 441 took 0.28639 seconds
Frame number 442 took 0.28399 seconds
Frame number 443 took 0.28124 seconds
Frame number 444 took 0.29253 seconds
Frame number 445 took 0.28448 seconds
Frame number 446 took 0.28451 seconds
Frame number 447 took 0.28002 seconds
Frame number 448 took 0.28403 seconds
Frame number 449 took 0.28084 seconds
Frame number 450 took 0.28517 seconds
Frame number 451 took 0.28547 seconds
Frame number 452 took 0.28482 seconds
Frame number 453 took 0.28425 seconds
Frame number 454 took 0.28362 seconds
Frame number 455 took 0.28363 seconds
Frame number 456 took 0.28389 seconds
Frame number 457 took 0.28525 seconds
Frame number 458 took 0.28030 seconds
Frame number 459 took 0.28581 seconds
Frame number 460 took 0.28566 seconds
Frame number 461 took 0.28429 seconds
Frame number 462 took 0.28112 seconds
Frame number 463 took 0.29488 seconds
Frame number 464 took 0.28477 seconds
Frame number 465 took 0.28046 seconds
Frame number 466 took 0.28188 seconds
Frame number 467 took 0.30430 seconds
Frame number 468 took 0.29189 seconds
Frame number 469 took 0.28744 seconds
Frame number 470 took 0.29079 seconds
Frame number 471 took 0.29167 seconds
Frame number 472 took 0.28232 seconds
Frame number 473 took 0.28225 seconds
Frame number 474 took 0.28513 seconds
Frame number 475 took 0.28421 seconds
Frame number 476 took 0.28111 seconds
Frame number 477 took 0.28141 seconds
Frame number 478 took 0.28410 seconds
Frame number 479 took 0.28299 seconds
Frame number 480 took 0.28371 seconds
Frame number 481 took 0.28478 seconds
Frame number 482 took 0.28366 seconds
Frame number 483 took 0.28398 seconds
Frame number 484 took 0.28656 seconds
Frame number 485 took 0.28441 seconds
Frame number 486 took 0.28336 seconds
Frame number 487 took 0.28206 seconds
Frame number 488 took 0.28605 seconds
Frame number 489 took 0.28500 seconds
Frame number 490 took 0.28582 seconds
Frame number 491 took 0.28510 seconds
Frame number 492 took 0.28411 seconds
Frame number 493 took 0.28051 seconds
Frame number 494 took 0.28275 seconds
Frame number 495 took 0.28497 seconds
Frame number 496 took 0.28270 seconds
Frame number 497 took 0.29587 seconds
Frame number 498 took 0.28226 seconds
Frame number 499 took 0.28232 seconds
Frame number 500 took 0.28405 seconds
Frame number 501 took 0.28243 seconds
Frame number 502 took 0.28481 seconds
Frame number 503 took 0.28029 seconds
Frame number 504 took 0.27936 seconds
Frame number 505 took 0.28082 seconds
Frame number 506 took 0.28182 seconds
Frame number 507 took 0.28221 seconds
Frame number 508 took 0.28344 seconds
Frame number 509 took 0.28629 seconds
Frame number 510 took 0.28458 seconds
Frame number 511 took 0.28489 seconds
Frame number 512 took 0.28251 seconds
Frame number 513 took 0.28448 seconds
Frame number 514 took 0.28538 seconds
Frame number 515 took 0.28534 seconds
Frame number 516 took 0.28393 seconds
Frame number 517 took 0.28128 seconds
</code></pre></div></div>

<h2 id="-fps-results"> FPS results</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'Total number of frames'</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Total amount of time {:.5f} seconds'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'FPS:'</span><span class="p">,</span> <span class="nb">round</span><span class="p">((</span><span class="n">f</span> <span class="o">/</span> <span class="n">t</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Total number of frames 517
Total amount of time 152.90212 seconds
FPS: 3.4
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Saving locally without committing
</span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">FileLink</span>

<span class="n">FileLink</span><span class="p">(</span><span class="s">'result.mp4'</span><span class="p">)</span>

</code></pre></div></div>

<p><a href="result.mp4" target="_blank">result.mp4</a><br /></p>

<h1 id="-reading-input-images"> Reading input images</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reading image with OpenCV library
# In this way image is opened already as numpy array
# WARNING! OpenCV by default reads images in BGR format
# image_BGR = cv2.imread('../input/videofortesting/traffic_sign.jpg')
# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_1.png')
# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_2.png')
# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_3.png')
# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_4.png')
# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_1.png')
# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_2.png')
# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_3.png')
</span><span class="n">image_BGR</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'../input/videofortesting/ts_final_1.png'</span><span class="p">)</span>

<span class="c1"># Check point
# Showing image shape
</span><span class="k">print</span><span class="p">(</span><span class="s">'Image shape:'</span><span class="p">,</span> <span class="n">image_BGR</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># tuple of (731, 1092, 3)
</span>
<span class="c1"># Getting spatial dimension of input image
</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">image_BGR</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># Slicing from tuple only first two elements
</span>
<span class="c1"># Check point
# Showing height an width of image
</span><span class="k">print</span><span class="p">(</span><span class="s">'Image height={0} and width={1}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>  <span class="c1"># 731 1092
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Image shape: (720, 1280, 3)
Image height=720 and width=1280
</code></pre></div></div>

<h1 id="-processing-single-image"> Processing single image</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Variable for counting total processing time
</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Blob from current frame
</span><span class="n">blob</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">blobFromImage</span><span class="p">(</span><span class="n">image_BGR</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="p">(</span><span class="mi">416</span><span class="p">,</span> <span class="mi">416</span><span class="p">),</span> <span class="n">swapRB</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">crop</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Forward pass with blob through output layers
</span><span class="n">network</span><span class="p">.</span><span class="n">setInput</span><span class="p">(</span><span class="n">blob</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">output_from_network</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">layers_names_output</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

<span class="c1"># Time
</span><span class="n">t</span> <span class="o">+=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Total amount of time {:.5f} seconds'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>

<span class="c1"># Lists for detected bounding boxes, confidences and class's number
</span><span class="n">bounding_boxes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">confidences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">class_numbers</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Going through all output layers after feed forward pass
</span><span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">output_from_network</span><span class="p">:</span>
    <span class="c1"># Going through all detections from current output layer
</span>    <span class="k">for</span> <span class="n">detected_objects</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
        <span class="c1"># Getting 80 classes' probabilities for current detected object
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">detected_objects</span><span class="p">[</span><span class="mi">5</span><span class="p">:]</span>
        <span class="c1"># Getting index of the class with the maximum value of probability
</span>        <span class="n">class_current</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="c1"># Getting value of probability for defined class
</span>        <span class="n">confidence_current</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">class_current</span><span class="p">]</span>

        <span class="c1"># Eliminating weak predictions by minimum probability
</span>        <span class="k">if</span> <span class="n">confidence_current</span> <span class="o">&gt;</span> <span class="n">probability_minimum</span><span class="p">:</span>
            <span class="c1"># Scaling bounding box coordinates to the initial frame size
</span>            <span class="n">box_current</span> <span class="o">=</span> <span class="n">detected_objects</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">])</span>

            <span class="c1"># Getting top left corner coordinates
</span>            <span class="n">x_center</span><span class="p">,</span> <span class="n">y_center</span><span class="p">,</span> <span class="n">box_width</span><span class="p">,</span> <span class="n">box_height</span> <span class="o">=</span> <span class="n">box_current</span>
            <span class="n">x_min</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">x_center</span> <span class="o">-</span> <span class="p">(</span><span class="n">box_width</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
            <span class="n">y_min</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">y_center</span> <span class="o">-</span> <span class="p">(</span><span class="n">box_height</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>

            <span class="c1"># Adding results into prepared lists
</span>            <span class="n">bounding_boxes</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">box_width</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">box_height</span><span class="p">)])</span>
            <span class="n">confidences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">confidence_current</span><span class="p">))</span>
            <span class="n">class_numbers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">class_current</span><span class="p">)</span>
                

<span class="c1"># Implementing non-maximum suppression of given bounding boxes
</span><span class="n">results</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">NMSBoxes</span><span class="p">(</span><span class="n">bounding_boxes</span><span class="p">,</span> <span class="n">confidences</span><span class="p">,</span> <span class="n">probability_minimum</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>

<span class="c1"># Checking if there is any detected object been left
</span><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># Going through indexes of results
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="n">flatten</span><span class="p">():</span>
        <span class="c1"># Bounding box coordinates, its width and height
</span>        <span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span> <span class="o">=</span> <span class="n">bounding_boxes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">bounding_boxes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">box_width</span><span class="p">,</span> <span class="n">box_height</span> <span class="o">=</span> <span class="n">bounding_boxes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">bounding_boxes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span>
            
            
        <span class="c1"># Cut fragment with Traffic Sign
</span>        <span class="n">c_ts</span> <span class="o">=</span> <span class="n">image_BGR</span><span class="p">[</span><span class="n">y_min</span><span class="p">:</span><span class="n">y_min</span><span class="o">+</span><span class="nb">int</span><span class="p">(</span><span class="n">box_height</span><span class="p">),</span> <span class="n">x_min</span><span class="p">:</span><span class="n">x_min</span><span class="o">+</span><span class="nb">int</span><span class="p">(</span><span class="n">box_width</span><span class="p">),</span> <span class="p">:]</span>
        <span class="c1"># print(c_ts.shape)
</span>            
        <span class="k">if</span> <span class="n">c_ts</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="ow">or</span> <span class="n">c_ts</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="mi">0</span><span class="p">,):</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Getting preprocessed blob with Traffic Sign of needed shape
</span>            <span class="n">blob_ts</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">blobFromImage</span><span class="p">(</span><span class="n">c_ts</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">swapRB</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">crop</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">blob_ts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">blob_ts</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">mean</span><span class="p">[</span><span class="s">'mean_image_rgb'</span><span class="p">]</span>
            <span class="n">blob_ts</span> <span class="o">=</span> <span class="n">blob_ts</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="c1"># plt.imshow(blob_ts[0, :, :, :])
</span>            <span class="c1"># plt.show()
</span>
            <span class="c1"># Feeding to the Keras CNN model to get predicted label among 43 classes
</span>            <span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">blob_ts</span><span class="p">)</span>

            <span class="c1"># Scores is given for image with 43 numbers of predictions for each class
</span>            <span class="c1"># Getting only one class with maximum value
</span>            <span class="n">prediction</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="s">'SignName'</span><span class="p">][</span><span class="n">prediction</span><span class="p">])</span>


            <span class="c1"># Colour for current bounding box
</span>            <span class="n">colour_box_current</span> <span class="o">=</span> <span class="n">colours</span><span class="p">[</span><span class="n">class_numbers</span><span class="p">[</span><span class="n">i</span><span class="p">]].</span><span class="n">tolist</span><span class="p">()</span>
            
            <span class="c1"># Green BGR
</span>            <span class="n">colour_box_current</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">61</span><span class="p">]</span>
            
            <span class="c1"># Yellow BGR
#             colour_box_current = [0, 255, 255]
</span>
            <span class="c1"># Drawing bounding box on the original current frame
</span>            <span class="n">cv2</span><span class="p">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">image_BGR</span><span class="p">,</span> <span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="p">),</span>
                              <span class="p">(</span><span class="n">x_min</span> <span class="o">+</span> <span class="n">box_width</span><span class="p">,</span> <span class="n">y_min</span> <span class="o">+</span> <span class="n">box_height</span><span class="p">),</span>
                              <span class="n">colour_box_current</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

<span class="c1">#             # Preparing text with label and confidence for current bounding box
#             text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],
#                                                    confidences[i])
</span>            
<span class="c1">#             # Putting text with label and confidence on the original image
#             cv2.putText(image_BGR, text_box_current, (x_min, y_min - 15),
#                             cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)
</span>            
            <span class="k">if</span> <span class="n">prediction</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
                <span class="c1"># Preparing text with label and confidence for current bounding box
</span>                <span class="n">text_box_current</span> <span class="o">=</span> <span class="s">'{}: {:.4f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">'Speed limit 60'</span><span class="p">,</span> <span class="n">confidences</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

                <span class="c1"># Putting text with label and confidence on the original image
</span>                <span class="n">cv2</span><span class="p">.</span><span class="n">putText</span><span class="p">(</span><span class="n">image_BGR</span><span class="p">,</span> <span class="n">text_box_current</span><span class="p">,</span> <span class="p">(</span><span class="n">x_min</span> <span class="o">-</span> <span class="mi">110</span><span class="p">,</span> <span class="n">y_min</span> <span class="o">-</span> <span class="mi">10</span><span class="p">),</span>
                                <span class="n">cv2</span><span class="p">.</span><span class="n">FONT_HERSHEY_SIMPLEX</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">colour_box_current</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
                
            <span class="k">elif</span> <span class="n">prediction</span> <span class="o">==</span> <span class="mi">9</span><span class="p">:</span>            
                <span class="c1"># Preparing text with label and confidence for current bounding box
</span>                <span class="n">text_box_current</span> <span class="o">=</span> <span class="s">'{}: {:.4f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">'No overtaking'</span><span class="p">,</span> <span class="n">confidences</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

                <span class="c1"># Putting text with label and confidence on the original image
</span>                <span class="n">cv2</span><span class="p">.</span><span class="n">putText</span><span class="p">(</span><span class="n">image_BGR</span><span class="p">,</span> <span class="n">text_box_current</span><span class="p">,</span> <span class="p">(</span><span class="n">x_min</span> <span class="o">-</span> <span class="mi">110</span><span class="p">,</span> <span class="n">y_min</span> <span class="o">+</span> <span class="n">box_height</span> <span class="o">+</span> <span class="mi">30</span><span class="p">),</span>
                                <span class="n">cv2</span><span class="p">.</span><span class="n">FONT_HERSHEY_SIMPLEX</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">colour_box_current</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1">#             elif prediction == 17:            
#                 # Preparing text with label and confidence for current bounding box
#                 text_box_current = '{}: {:.4f}'.format('No entry', confidences[i])
</span>
<span class="c1">#                 # Putting text with label and confidence on the original image
#                 cv2.putText(image_BGR, text_box_current, (x_min - 170, y_min - 15),
#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)
</span>                
                
<span class="c1"># Saving image
</span><span class="n">cv2</span><span class="p">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s">'result.png'</span><span class="p">,</span> <span class="n">image_BGR</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Total amount of time 0.29843 seconds
Ahead only
No passing





True
</code></pre></div></div>

<h1 id="-showing-processed-image"> Showing processed image</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">35.0</span><span class="p">,</span> <span class="mf">35.0</span><span class="p">)</span> <span class="c1"># Setting default size of plots
</span>
<span class="n">image_BGR</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'/kaggle/working/result.png'</span><span class="p">)</span>

<span class="c1"># Showing image shape
</span><span class="k">print</span><span class="p">(</span><span class="s">'Image shape:'</span><span class="p">,</span> <span class="n">image_BGR</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># tuple of (800, 1360, 3)
</span>
<span class="c1"># Getting spatial dimension of input image
</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">image_BGR</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># Slicing from tuple only first two elements
</span>
<span class="c1"># Showing height an width of image
</span><span class="k">print</span><span class="p">(</span><span class="s">'Image height={0} and width={1}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>  <span class="c1"># 800 1360
</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image_BGR</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="c1"># plt.title('Keras Visualization', fontsize=18)
</span>
<span class="c1"># Showing the plot
</span><span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Image shape: (720, 1280, 3)
Image height=720 and width=1280
</code></pre></div></div>

<p><img src="/assets/output_28_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Saving locally without committing
</span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">FileLink</span>

<span class="n">FileLink</span><span class="p">(</span><span class="s">'result.png'</span><span class="p">)</span>

</code></pre></div></div>

<p><a href="result.png" target="_blank">result.png</a><br /></p>

<h1 id="-example-of-the-result"> Example of the result</h1>

<p><img src="https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3400968%2Fa57f58b38e3caab6fbf72169895f5074%2Fresult.gif?generation=1585955236302060&amp;alt=media" alt="" /></p>


    <div class="share">
  
    
      
      
      
      

      

      <a class="button" href="https://twitter.com/intent/tweet/?url=http://localhost:4000/ml_project/2021/03/22/traffic-signs-detection/&text=Traffic%20Signs%20Detection&via=sonalithote" style="background: #0d94e7">Twitter&nbsp; <svg width="16" height="16" class="icon  icon--twitter" role="img" alt="twitter"><title>twitter</title><use xlink:href="#twitter" fill="CurrentColor"></use></svg>
</a>

    
  
    
      
      
      
      

      

      <a class="button" href="https://facebook.com/sharer/sharer.php?u=http://localhost:4000/ml_project/2021/03/22/traffic-signs-detection/" style="background: #3B5998">facebook&nbsp; <svg width="16" height="16" class="icon  icon--facebook" role="img" alt="facebook"><title>facebook</title><use xlink:href="#facebook" fill="CurrentColor"></use></svg>
</a>

    
  
    
      
      
      
      

      

      <a class="button" href="mailto:?subject=Traffic Signs Detection&body=Hey, check out this: http://localhost:4000/ml_project/2021/03/22/traffic-signs-detection/" style="background: true">Email&nbsp; <svg width="16" height="16" class="icon  icon--email" role="img" alt="email"><title>email</title><use xlink:href="#email" fill="CurrentColor"></use></svg>
</a>

    
  
</div>


    


  </article>

  

</main>

<footer class="footer">
  <div class="container">
    <div class="copyright  typeset">
      <small class="small">&copy; Alembic 2024</small>
    </div>

    
<nav class="nav  nav--footer">
  <ul class="list list--nav">
    

      

      <li class="item  item--nav">
        <a href="https://sonalithote.github.io">Created by Sonali Thote</a>
      </li>
    
  </ul>
</nav>


  </div>
</footer>



    <script type="text/javascript">
(() => {
  const registerServiceWorker = () => {
    if (!navigator.serviceWorker) {
      return;
    }

    navigator.serviceWorker
      .register("/sw.js")
      .then(registration => {
        console.log("Service Worker: registered");
      })
      .catch(err => {
        console.log("Service Worker: registration failed ", err);
      });
  };

  registerServiceWorker();
})();
</script>


    <!-- Overwrite this file with code you want before the closing body tag -->

  </body>

</html>
