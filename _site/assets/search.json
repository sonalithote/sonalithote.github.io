

[
  
  
    
    
      {
        "title": "Traffic Signs Detection",
        "excerpt": "The primary objectives for any recognition system include detection (ascertaining the location and size of an object within an input image) and classification (assigning the detected objects into specific subclasses). Typically, a singular detection/classification model, such as YOLO or SSD, is employed for both tasks, where input images are annotated with bounding boxes and their corresponding classes. However, the labelling and training of such datasets demand considerable time and effort. Consequently, the principal aim of this project is to identify a single main class (signs) and to incorporate a custom-built Convolutional Neural Network for the classification of the detected objects into subclasses (for instance, speed limits, stop signs, etc.). This approach necessitates training a detection model only once to recognize one main class, while allowing for the training of multiple classification models to categorize detected objects based on the specific requirements of the task.\n",
        "content": "Traffic Signs Detection with OpenCV &amp; Keras\n\nLeveraging YOLO v3, OpenCV, and Keras for traffic signs detection offers a cutting-edge solution in computer vision, crucial for enhancing autonomous vehicle technologies. YOLO v3 excels in real-time object detection, identifying traffic signs quickly and accurately. OpenCV aids in processing these images, while Keras specializes in classifying the detected signs into their respective categories, such as speed limits and stop signs. This combination delivers a powerful, efficient system for traffic sign detection, crucial for improving the safety and reliability of autonomous navigation systems.\n\n\n  Initially, a model trained within the Darknet framework utilizes the OpenCV dnn library to identify Traffic Signs across four distinct categories.\n  Subsequently, another model, developed using Keras, classifies the segmented portions of these Traffic Signs into one of 43 different classes.\n  Although the results are currently experimental, they hold potential for future enhancements.\n\n\nExample of detections on video are shown below. Trained weights can be found in the course mentioned above.\n\n\n\nImporting required libraries\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport time\nfrom timeit import default_timer as timer\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom keras.models import load_model\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('../input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nprint(os.listdir('archive/'))\n\n# Any results we write to the current directory are saved as output\n\n\n\n['test.pickle', 'data0.pickle', 'data2.pickle', 'std_gray.pickle', 'mean_image_rgb.pickle', 'data6.pickle', 'datasets_preparing.py', 'data8.pickle', 'data4.pickle', 'label_names.csv', 'data1.pickle', 'valid.pickle', 'std_rgb.pickle', 'data3.pickle', 'train.pickle', 'data7.pickle', 'mean_image_gray.pickle', 'data5.pickle', 'labels.pickle']\n\n\n📂 Loading labels\n\n# Reading csv file with labels' names\n# Loading two columns [0, 1] into Pandas dataFrame\nlabels = pd.read_csv('../input/traffic-signs-preprocessed/label_names.csv')\n\n# Check point\n# Showing first 5 rows from the dataFrame\nprint(labels.head())\nprint()\n\n# To locate by class number use one of the following\n# ***.iloc[0][1] - returns element on the 0 column and 1 row\nprint(labels.iloc[0][1])  # Speed limit (20km/h)\n# ***['SignName'][1] - returns element on the column with name 'SignName' and 1 row\nprint(labels['SignName'][1]) # Speed limit (30km/h)\n\n\n\n   ClassId              SignName\n0        0  Speed limit (20km/h)\n1        1  Speed limit (30km/h)\n2        2  Speed limit (50km/h)\n3        3  Speed limit (60km/h)\n4        4  Speed limit (70km/h)\n\nSpeed limit (20km/h)\nSpeed limit (30km/h)\n\n\n📂 Loading trained Keras CNN model for Classification\n\n# Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes\nmodel = load_model('../input/traffic-signs-classification-with-cnn/model-23x23.h5')\n\n# Loading mean image to use for preprocessing further\n# Opening file for reading in binary mode\nwith open('../input/traffic-signs-preprocessed/mean_image_rgb.pickle', 'rb') as f:\n    mean = pickle.load(f, encoding='latin1')  # dictionary type\n    \nprint(mean['mean_image_rgb'].shape)  # (3, 32, 32)\n\n\n\n(3, 32, 32)\n\n\n💠 Loading YOLO v3 network by OpenCV dnn library\n\nLoading trained weights and cfg file into the Network\n\n# Trained weights can be found in the course mentioned above\npath_to_weights = '../input/trained-traffic-signs-detector-based-on-yolo-v3/yolov3_ts_train_5000.weights'\npath_to_cfg = '../input/traffic-signs-dataset-in-yolo-format/yolov3_ts_test.cfg'\n\n# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\nnetwork = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\n\n# To use with GPU\nnetwork.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnetwork.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n\n\n\nGetting output layers where detections are made\n\n# Getting names of all YOLO v3 layers\nlayers_all = network.getLayerNames()\n\n# Check point\n# print(layers_all)\n\n# Getting only detection YOLO v3 layers that are 82, 94 and 106\nlayers_names_output = [layers_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n\n# Check point\nprint()\nprint(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n\n\n\n['yolo_82', 'yolo_94', 'yolo_106']\n\n\nSetting probability, threshold and colour for bounding boxes\n\n# Minimum probability to eliminate weak detections\nprobability_minimum = 0.2\n\n# Setting threshold to filtering weak bounding boxes by non-maximum suppression\nthreshold = 0.2\n\n# Generating colours for bounding boxes\n# randint(low, high=None, size=None, dtype='l')\ncolours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n\n# Check point\nprint(type(colours))  # &lt;class 'numpy.ndarray'&gt;\nprint(colours.shape)  # (43, 3)\nprint(colours[0])  # [25  65 200]\n\n\n\n&lt;class 'numpy.ndarray'&gt;\n(43, 3)\n[203  49  61]\n\n\n🎬 Reading input video\n\n# Reading video from a file by VideoCapture object\n# video = cv2.VideoCapture('../input/traffic-signs-dataset-in-yolo-format/traffic-sign-to-test.mp4')\n# video = cv2.VideoCapture('../input/videofortesting/ts_video_1.mp4')\nvideo = cv2.VideoCapture('../input/videofortesting/ts_video_6.mp4')\n\n# Writer that will be used to write processed frames\nwriter = None\n\n# Variables for spatial dimensions of the frames\nh, w = None, None\n\n\n\n➿ Processing frames in the loop\n\n%matplotlib inline\n\n# Setting default size of plots\nplt.rcParams['figure.figsize'] = (3, 3)\n\n# Variable for counting total amount of frames\nf = 0\n\n# Variable for counting total processing time\nt = 0\n\n# Catching frames in the loop\nwhile True:\n    # Capturing frames one-by-one\n    ret, frame = video.read()\n\n    # If the frame was not retrieved\n    if not ret:\n        break\n       \n    # Getting spatial dimensions of the frame for the first time\n    if w is None or h is None:\n        # Slicing two elements from tuple\n        h, w = frame.shape[:2]\n\n    # Blob from current frame\n    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n\n    # Forward pass with blob through output layers\n    network.setInput(blob)\n    start = time.time()\n    output_from_network = network.forward(layers_names_output)\n    end = time.time()\n\n    # Increasing counters\n    f += 1\n    t += end - start\n\n    # Spent time for current frame\n    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n\n    # Lists for detected bounding boxes, confidences and class's number\n    bounding_boxes = []\n    confidences = []\n    class_numbers = []\n\n    # Going through all output layers after feed forward pass\n    for result in output_from_network:\n        # Going through all detections from current output layer\n        for detected_objects in result:\n            # Getting 80 classes' probabilities for current detected object\n            scores = detected_objects[5:]\n            # Getting index of the class with the maximum value of probability\n            class_current = np.argmax(scores)\n            # Getting value of probability for defined class\n            confidence_current = scores[class_current]\n\n            # Eliminating weak predictions by minimum probability\n            if confidence_current &gt; probability_minimum:\n                # Scaling bounding box coordinates to the initial frame size\n                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n                # Getting top left corner coordinates\n                x_center, y_center, box_width, box_height = box_current\n                x_min = int(x_center - (box_width / 2))\n                y_min = int(y_center - (box_height / 2))\n\n                # Adding results into prepared lists\n                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n                confidences.append(float(confidence_current))\n                class_numbers.append(class_current)\n                \n\n    # Implementing non-maximum suppression of given bounding boxes\n    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n\n    # Checking if there is any detected object been left\n    if len(results) &gt; 0:\n        # Going through indexes of results\n        for i in results.flatten():\n            # Bounding box coordinates, its width and height\n            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n            \n            \n            # Cut fragment with Traffic Sign\n            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n            # print(c_ts.shape)\n            \n            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n                pass\n            else:\n                # Getting preprocessed blob with Traffic Sign of needed shape\n                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n                # plt.imshow(blob_ts[0, :, :, :])\n                # plt.show()\n\n                # Feeding to the Keras CNN model to get predicted label among 43 classes\n                scores = model.predict(blob_ts)\n\n                # Scores is given for image with 43 numbers of predictions for each class\n                # Getting only one class with maximum value\n                prediction = np.argmax(scores)\n                # print(labels['SignName'][prediction])\n\n\n                # Colour for current bounding box\n                colour_box_current = colours[class_numbers[i]].tolist()\n\n                # Drawing bounding box on the original current frame\n                cv2.rectangle(frame, (x_min, y_min),\n                              (x_min + box_width, y_min + box_height),\n                              colour_box_current, 2)\n\n                # Preparing text with label and confidence for current bounding box\n                text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n                                                       confidences[i])\n\n                # Putting text with label and confidence on the original image\n                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n\n\n    # Initializing writer only once\n    if writer is None:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\n        # Writing current processed frame into the video file\n        writer = cv2.VideoWriter('result.mp4', fourcc, 25,\n                                 (frame.shape[1], frame.shape[0]), True)\n\n    # Write processed current frame to the file\n    writer.write(frame)\n\n\n# Releasing video reader and writer\nvideo.release()\nwriter.release()\n\n\n\nFrame number 1 took 5.52782 seconds\nFrame number 2 took 0.30848 seconds\nFrame number 3 took 0.28469 seconds\nFrame number 4 took 0.28405 seconds\nFrame number 5 took 0.28431 seconds\nFrame number 6 took 0.28376 seconds\nFrame number 7 took 0.28409 seconds\nFrame number 8 took 0.28311 seconds\nFrame number 9 took 0.28223 seconds\nFrame number 10 took 0.28190 seconds\nFrame number 11 took 0.28196 seconds\nFrame number 12 took 0.28338 seconds\nFrame number 13 took 0.28229 seconds\nFrame number 14 took 0.28356 seconds\nFrame number 15 took 0.28600 seconds\nFrame number 16 took 0.29676 seconds\nFrame number 17 took 0.28499 seconds\nFrame number 18 took 0.28229 seconds\nFrame number 19 took 0.28268 seconds\nFrame number 20 took 0.28224 seconds\nFrame number 21 took 0.28413 seconds\nFrame number 22 took 0.28356 seconds\nFrame number 23 took 0.28463 seconds\nFrame number 24 took 0.28319 seconds\nFrame number 25 took 0.28683 seconds\nFrame number 26 took 0.28725 seconds\nFrame number 27 took 0.28898 seconds\nFrame number 28 took 0.28659 seconds\nFrame number 29 took 0.28389 seconds\nFrame number 30 took 0.28378 seconds\nFrame number 31 took 0.28475 seconds\nFrame number 32 took 0.28251 seconds\nFrame number 33 took 0.28269 seconds\nFrame number 34 took 0.28348 seconds\nFrame number 35 took 0.28382 seconds\nFrame number 36 took 0.28517 seconds\nFrame number 37 took 0.28654 seconds\nFrame number 38 took 0.28655 seconds\nFrame number 39 took 0.28563 seconds\nFrame number 40 took 0.28403 seconds\nFrame number 41 took 0.28611 seconds\nFrame number 42 took 0.28182 seconds\nFrame number 43 took 0.29107 seconds\nFrame number 44 took 0.28453 seconds\nFrame number 45 took 0.28377 seconds\nFrame number 46 took 0.28447 seconds\nFrame number 47 took 0.28513 seconds\nFrame number 48 took 0.28379 seconds\nFrame number 49 took 0.28776 seconds\nFrame number 50 took 0.28735 seconds\nFrame number 51 took 0.28501 seconds\nFrame number 52 took 0.28487 seconds\nFrame number 53 took 0.28414 seconds\nFrame number 54 took 0.28307 seconds\nFrame number 55 took 0.28253 seconds\nFrame number 56 took 0.28466 seconds\nFrame number 57 took 0.28470 seconds\nFrame number 58 took 0.28420 seconds\nFrame number 59 took 0.28372 seconds\nFrame number 60 took 0.28218 seconds\nFrame number 61 took 0.28171 seconds\nFrame number 62 took 0.28202 seconds\nFrame number 63 took 0.28421 seconds\nFrame number 64 took 0.28256 seconds\nFrame number 65 took 0.28365 seconds\nFrame number 66 took 0.28422 seconds\nFrame number 67 took 0.28526 seconds\nFrame number 68 took 0.28542 seconds\nFrame number 69 took 0.28809 seconds\nFrame number 70 took 0.30996 seconds\nFrame number 71 took 0.39311 seconds\nFrame number 72 took 0.29010 seconds\nFrame number 73 took 0.28246 seconds\nFrame number 74 took 0.28550 seconds\nFrame number 75 took 0.28487 seconds\nFrame number 76 took 0.28557 seconds\nFrame number 77 took 0.28503 seconds\nFrame number 78 took 0.28439 seconds\nFrame number 79 took 0.28619 seconds\nFrame number 80 took 0.28490 seconds\nFrame number 81 took 0.28697 seconds\nFrame number 82 took 0.28585 seconds\nFrame number 83 took 0.28697 seconds\nFrame number 84 took 0.28652 seconds\nFrame number 85 took 0.28533 seconds\nFrame number 86 took 0.28449 seconds\nFrame number 87 took 0.28467 seconds\nFrame number 88 took 0.28317 seconds\nFrame number 89 took 0.28570 seconds\nFrame number 90 took 0.28561 seconds\nFrame number 91 took 0.28517 seconds\nFrame number 92 took 0.28556 seconds\nFrame number 93 took 0.28932 seconds\nFrame number 94 took 0.28604 seconds\nFrame number 95 took 0.28409 seconds\nFrame number 96 took 0.29276 seconds\nFrame number 97 took 0.28508 seconds\nFrame number 98 took 0.28641 seconds\nFrame number 99 took 0.28600 seconds\nFrame number 100 took 0.28778 seconds\nFrame number 101 took 0.28482 seconds\nFrame number 102 took 0.28302 seconds\nFrame number 103 took 0.28462 seconds\nFrame number 104 took 0.28550 seconds\nFrame number 105 took 0.28799 seconds\nFrame number 106 took 0.28656 seconds\nFrame number 107 took 0.28632 seconds\nFrame number 108 took 0.28320 seconds\nFrame number 109 took 0.28345 seconds\nFrame number 110 took 0.28199 seconds\nFrame number 111 took 0.28399 seconds\nFrame number 112 took 0.28442 seconds\nFrame number 113 took 0.28563 seconds\nFrame number 114 took 0.28655 seconds\nFrame number 115 took 0.28611 seconds\nFrame number 116 took 0.28686 seconds\nFrame number 117 took 0.28606 seconds\nFrame number 118 took 0.28554 seconds\nFrame number 119 took 0.28580 seconds\nFrame number 120 took 0.28680 seconds\nFrame number 121 took 0.28843 seconds\nFrame number 122 took 0.29228 seconds\nFrame number 123 took 0.29320 seconds\nFrame number 124 took 0.28505 seconds\nFrame number 125 took 0.28440 seconds\nFrame number 126 took 0.28295 seconds\nFrame number 127 took 0.28477 seconds\nFrame number 128 took 0.28561 seconds\nFrame number 129 took 0.28503 seconds\nFrame number 130 took 0.28481 seconds\nFrame number 131 took 0.28668 seconds\nFrame number 132 took 0.28682 seconds\nFrame number 133 took 0.28491 seconds\nFrame number 134 took 0.28558 seconds\nFrame number 135 took 0.28669 seconds\nFrame number 136 took 0.28582 seconds\nFrame number 137 took 0.28552 seconds\nFrame number 138 took 0.28360 seconds\nFrame number 139 took 0.28467 seconds\nFrame number 140 took 0.28590 seconds\nFrame number 141 took 0.28458 seconds\nFrame number 142 took 0.28605 seconds\nFrame number 143 took 0.28407 seconds\nFrame number 144 took 0.28660 seconds\nFrame number 145 took 0.28519 seconds\nFrame number 146 took 0.28497 seconds\nFrame number 147 took 0.28567 seconds\nFrame number 148 took 0.28722 seconds\nFrame number 149 took 0.30203 seconds\nFrame number 150 took 0.39440 seconds\nFrame number 151 took 0.29713 seconds\nFrame number 152 took 0.28998 seconds\nFrame number 153 took 0.28315 seconds\nFrame number 154 took 0.28381 seconds\nFrame number 155 took 0.28484 seconds\nFrame number 156 took 0.28455 seconds\nFrame number 157 took 0.28462 seconds\nFrame number 158 took 0.28355 seconds\nFrame number 159 took 0.28561 seconds\nFrame number 160 took 0.28295 seconds\nFrame number 161 took 0.28152 seconds\nFrame number 162 took 0.28430 seconds\nFrame number 163 took 0.28459 seconds\nFrame number 164 took 0.28397 seconds\nFrame number 165 took 0.28378 seconds\nFrame number 166 took 0.28418 seconds\nFrame number 167 took 0.28647 seconds\nFrame number 168 took 0.28554 seconds\nFrame number 169 took 0.28408 seconds\nFrame number 170 took 0.28628 seconds\nFrame number 171 took 0.28269 seconds\nFrame number 172 took 0.28406 seconds\nFrame number 173 took 0.28388 seconds\nFrame number 174 took 0.28510 seconds\nFrame number 175 took 0.28671 seconds\nFrame number 176 took 0.29481 seconds\nFrame number 177 took 0.28572 seconds\nFrame number 178 took 0.28461 seconds\nFrame number 179 took 0.28354 seconds\nFrame number 180 took 0.28381 seconds\nFrame number 181 took 0.28487 seconds\nFrame number 182 took 0.28755 seconds\nFrame number 183 took 0.28590 seconds\nFrame number 184 took 0.28326 seconds\nFrame number 185 took 0.28332 seconds\nFrame number 186 took 0.28574 seconds\nFrame number 187 took 0.28746 seconds\nFrame number 188 took 0.28684 seconds\nFrame number 189 took 0.28440 seconds\nFrame number 190 took 0.28410 seconds\nFrame number 191 took 0.28283 seconds\nFrame number 192 took 0.28129 seconds\nFrame number 193 took 0.28304 seconds\nFrame number 194 took 0.28127 seconds\nFrame number 195 took 0.28551 seconds\nFrame number 196 took 0.28479 seconds\nFrame number 197 took 0.28270 seconds\nFrame number 198 took 0.28455 seconds\nFrame number 199 took 0.28374 seconds\nFrame number 200 took 0.28207 seconds\nFrame number 201 took 0.28154 seconds\nFrame number 202 took 0.28349 seconds\nFrame number 203 took 0.29308 seconds\nFrame number 204 took 0.28364 seconds\nFrame number 205 took 0.28461 seconds\nFrame number 206 took 0.28661 seconds\nFrame number 207 took 0.28589 seconds\nFrame number 208 took 0.28265 seconds\nFrame number 209 took 0.28450 seconds\nFrame number 210 took 0.28276 seconds\nFrame number 211 took 0.28015 seconds\nFrame number 212 took 0.28183 seconds\nFrame number 213 took 0.28343 seconds\nFrame number 214 took 0.28340 seconds\nFrame number 215 took 0.28382 seconds\nFrame number 216 took 0.28503 seconds\nFrame number 217 took 0.28606 seconds\nFrame number 218 took 0.28351 seconds\nFrame number 219 took 0.28184 seconds\nFrame number 220 took 0.28242 seconds\nFrame number 221 took 0.28519 seconds\nFrame number 222 took 0.28396 seconds\nFrame number 223 took 0.29134 seconds\nFrame number 224 took 0.28385 seconds\nFrame number 225 took 0.28655 seconds\nFrame number 226 took 0.28653 seconds\nFrame number 227 took 0.28642 seconds\nFrame number 228 took 0.29651 seconds\nFrame number 229 took 0.29838 seconds\nFrame number 230 took 0.38873 seconds\nFrame number 231 took 0.28324 seconds\nFrame number 232 took 0.28234 seconds\nFrame number 233 took 0.28309 seconds\nFrame number 234 took 0.28333 seconds\nFrame number 235 took 0.28514 seconds\nFrame number 236 took 0.28152 seconds\nFrame number 237 took 0.28373 seconds\nFrame number 238 took 0.28316 seconds\nFrame number 239 took 0.28187 seconds\nFrame number 240 took 0.28413 seconds\nFrame number 241 took 0.28420 seconds\nFrame number 242 took 0.28467 seconds\nFrame number 243 took 0.28419 seconds\nFrame number 244 took 0.28625 seconds\nFrame number 245 took 0.28635 seconds\nFrame number 246 took 0.28186 seconds\nFrame number 247 took 0.28371 seconds\nFrame number 248 took 0.28258 seconds\nFrame number 249 took 0.28080 seconds\nFrame number 250 took 0.28274 seconds\nFrame number 251 took 0.28412 seconds\nFrame number 252 took 0.28426 seconds\nFrame number 253 took 0.28682 seconds\nFrame number 254 took 0.28167 seconds\nFrame number 255 took 0.28321 seconds\nFrame number 256 took 0.29472 seconds\nFrame number 257 took 0.28530 seconds\nFrame number 258 took 0.28456 seconds\nFrame number 259 took 0.28545 seconds\nFrame number 260 took 0.28461 seconds\nFrame number 261 took 0.28277 seconds\nFrame number 262 took 0.28404 seconds\nFrame number 263 took 0.28606 seconds\nFrame number 264 took 0.28602 seconds\nFrame number 265 took 0.28313 seconds\nFrame number 266 took 0.28395 seconds\nFrame number 267 took 0.28183 seconds\nFrame number 268 took 0.28174 seconds\nFrame number 269 took 0.28410 seconds\nFrame number 270 took 0.28458 seconds\nFrame number 271 took 0.28675 seconds\nFrame number 272 took 0.28688 seconds\nFrame number 273 took 0.28503 seconds\nFrame number 274 took 0.28500 seconds\nFrame number 275 took 0.28310 seconds\nFrame number 276 took 0.28119 seconds\nFrame number 277 took 0.28336 seconds\nFrame number 278 took 0.28553 seconds\nFrame number 279 took 0.28352 seconds\nFrame number 280 took 0.28545 seconds\nFrame number 281 took 0.28195 seconds\nFrame number 282 took 0.28735 seconds\nFrame number 283 took 0.29508 seconds\nFrame number 284 took 0.28249 seconds\nFrame number 285 took 0.28274 seconds\nFrame number 286 took 0.28265 seconds\nFrame number 287 took 0.28077 seconds\nFrame number 288 took 0.28335 seconds\nFrame number 289 took 0.28146 seconds\nFrame number 290 took 0.28254 seconds\nFrame number 291 took 0.28325 seconds\nFrame number 292 took 0.28674 seconds\nFrame number 293 took 0.28403 seconds\nFrame number 294 took 0.28442 seconds\nFrame number 295 took 0.28130 seconds\nFrame number 296 took 0.28412 seconds\nFrame number 297 took 0.28505 seconds\nFrame number 298 took 0.28399 seconds\nFrame number 299 took 0.28320 seconds\nFrame number 300 took 0.28322 seconds\nFrame number 301 took 0.28711 seconds\nFrame number 302 took 0.28616 seconds\nFrame number 303 took 0.28305 seconds\nFrame number 304 took 0.28422 seconds\nFrame number 305 took 0.28305 seconds\nFrame number 306 took 0.28035 seconds\nFrame number 307 took 0.29199 seconds\nFrame number 308 took 0.28817 seconds\nFrame number 309 took 0.29844 seconds\nFrame number 310 took 0.29078 seconds\nFrame number 311 took 0.28241 seconds\nFrame number 312 took 0.28323 seconds\nFrame number 313 took 0.28173 seconds\nFrame number 314 took 0.28147 seconds\nFrame number 315 took 0.28425 seconds\nFrame number 316 took 0.28448 seconds\nFrame number 317 took 0.28600 seconds\nFrame number 318 took 0.28327 seconds\nFrame number 319 took 0.28452 seconds\nFrame number 320 took 0.28637 seconds\nFrame number 321 took 0.28627 seconds\nFrame number 322 took 0.28629 seconds\nFrame number 323 took 0.28339 seconds\nFrame number 324 took 0.28367 seconds\nFrame number 325 took 0.28587 seconds\nFrame number 326 took 0.28509 seconds\nFrame number 327 took 0.28538 seconds\nFrame number 328 took 0.28441 seconds\nFrame number 329 took 0.28467 seconds\nFrame number 330 took 0.28634 seconds\nFrame number 331 took 0.28423 seconds\nFrame number 332 took 0.28498 seconds\nFrame number 333 took 0.28668 seconds\nFrame number 334 took 0.28574 seconds\nFrame number 335 took 0.28656 seconds\nFrame number 336 took 0.29757 seconds\nFrame number 337 took 0.28516 seconds\nFrame number 338 took 0.28614 seconds\nFrame number 339 took 0.28451 seconds\nFrame number 340 took 0.28847 seconds\nFrame number 341 took 0.28675 seconds\nFrame number 342 took 0.28578 seconds\nFrame number 343 took 0.28292 seconds\nFrame number 344 took 0.28233 seconds\nFrame number 345 took 0.28436 seconds\nFrame number 346 took 0.28223 seconds\nFrame number 347 took 0.28319 seconds\nFrame number 348 took 0.28307 seconds\nFrame number 349 took 0.28340 seconds\nFrame number 350 took 0.28312 seconds\nFrame number 351 took 0.28410 seconds\nFrame number 352 took 0.28347 seconds\nFrame number 353 took 0.28253 seconds\nFrame number 354 took 0.28212 seconds\nFrame number 355 took 0.28306 seconds\nFrame number 356 took 0.28282 seconds\nFrame number 357 took 0.28295 seconds\nFrame number 358 took 0.28414 seconds\nFrame number 359 took 0.28654 seconds\nFrame number 360 took 0.28786 seconds\nFrame number 361 took 0.28524 seconds\nFrame number 362 took 0.29588 seconds\nFrame number 363 took 0.29353 seconds\nFrame number 364 took 0.28240 seconds\nFrame number 365 took 0.28102 seconds\nFrame number 366 took 0.28119 seconds\nFrame number 367 took 0.28203 seconds\nFrame number 368 took 0.28123 seconds\nFrame number 369 took 0.28110 seconds\nFrame number 370 took 0.28499 seconds\nFrame number 371 took 0.28321 seconds\nFrame number 372 took 0.28100 seconds\nFrame number 373 took 0.28231 seconds\nFrame number 374 took 0.28319 seconds\nFrame number 375 took 0.28479 seconds\nFrame number 376 took 0.28427 seconds\nFrame number 377 took 0.28608 seconds\nFrame number 378 took 0.28534 seconds\nFrame number 379 took 0.28462 seconds\nFrame number 380 took 0.28592 seconds\nFrame number 381 took 0.28679 seconds\nFrame number 382 took 0.28434 seconds\nFrame number 383 took 0.28003 seconds\nFrame number 384 took 0.28291 seconds\nFrame number 385 took 0.31018 seconds\nFrame number 386 took 0.29572 seconds\nFrame number 387 took 0.29071 seconds\nFrame number 388 took 0.28215 seconds\nFrame number 389 took 0.29660 seconds\nFrame number 390 took 0.29125 seconds\nFrame number 391 took 0.28239 seconds\nFrame number 392 took 0.28199 seconds\nFrame number 393 took 0.28238 seconds\nFrame number 394 took 0.28447 seconds\nFrame number 395 took 0.28208 seconds\nFrame number 396 took 0.28237 seconds\nFrame number 397 took 0.28580 seconds\nFrame number 398 took 0.28642 seconds\nFrame number 399 took 0.28645 seconds\nFrame number 400 took 0.28367 seconds\nFrame number 401 took 0.28445 seconds\nFrame number 402 took 0.28423 seconds\nFrame number 403 took 0.28436 seconds\nFrame number 404 took 0.28554 seconds\nFrame number 405 took 0.28653 seconds\nFrame number 406 took 0.28459 seconds\nFrame number 407 took 0.28579 seconds\nFrame number 408 took 0.28109 seconds\nFrame number 409 took 0.28283 seconds\nFrame number 410 took 0.28278 seconds\nFrame number 411 took 0.28106 seconds\nFrame number 412 took 0.28101 seconds\nFrame number 413 took 0.28220 seconds\nFrame number 414 took 0.28197 seconds\nFrame number 415 took 0.28409 seconds\nFrame number 416 took 0.28440 seconds\nFrame number 417 took 0.29191 seconds\nFrame number 418 took 0.28376 seconds\nFrame number 419 took 0.28163 seconds\nFrame number 420 took 0.28463 seconds\nFrame number 421 took 0.28094 seconds\nFrame number 422 took 0.28145 seconds\nFrame number 423 took 0.28248 seconds\nFrame number 424 took 0.28267 seconds\nFrame number 425 took 0.28153 seconds\nFrame number 426 took 0.28141 seconds\nFrame number 427 took 0.28267 seconds\nFrame number 428 took 0.28160 seconds\nFrame number 429 took 0.28295 seconds\nFrame number 430 took 0.28292 seconds\nFrame number 431 took 0.28278 seconds\nFrame number 432 took 0.28456 seconds\nFrame number 433 took 0.28434 seconds\nFrame number 434 took 0.28545 seconds\nFrame number 435 took 0.28577 seconds\nFrame number 436 took 0.28626 seconds\nFrame number 437 took 0.28582 seconds\nFrame number 438 took 0.28445 seconds\nFrame number 439 took 0.28572 seconds\nFrame number 440 took 0.28439 seconds\nFrame number 441 took 0.28639 seconds\nFrame number 442 took 0.28399 seconds\nFrame number 443 took 0.28124 seconds\nFrame number 444 took 0.29253 seconds\nFrame number 445 took 0.28448 seconds\nFrame number 446 took 0.28451 seconds\nFrame number 447 took 0.28002 seconds\nFrame number 448 took 0.28403 seconds\nFrame number 449 took 0.28084 seconds\nFrame number 450 took 0.28517 seconds\nFrame number 451 took 0.28547 seconds\nFrame number 452 took 0.28482 seconds\nFrame number 453 took 0.28425 seconds\nFrame number 454 took 0.28362 seconds\nFrame number 455 took 0.28363 seconds\nFrame number 456 took 0.28389 seconds\nFrame number 457 took 0.28525 seconds\nFrame number 458 took 0.28030 seconds\nFrame number 459 took 0.28581 seconds\nFrame number 460 took 0.28566 seconds\nFrame number 461 took 0.28429 seconds\nFrame number 462 took 0.28112 seconds\nFrame number 463 took 0.29488 seconds\nFrame number 464 took 0.28477 seconds\nFrame number 465 took 0.28046 seconds\nFrame number 466 took 0.28188 seconds\nFrame number 467 took 0.30430 seconds\nFrame number 468 took 0.29189 seconds\nFrame number 469 took 0.28744 seconds\nFrame number 470 took 0.29079 seconds\nFrame number 471 took 0.29167 seconds\nFrame number 472 took 0.28232 seconds\nFrame number 473 took 0.28225 seconds\nFrame number 474 took 0.28513 seconds\nFrame number 475 took 0.28421 seconds\nFrame number 476 took 0.28111 seconds\nFrame number 477 took 0.28141 seconds\nFrame number 478 took 0.28410 seconds\nFrame number 479 took 0.28299 seconds\nFrame number 480 took 0.28371 seconds\nFrame number 481 took 0.28478 seconds\nFrame number 482 took 0.28366 seconds\nFrame number 483 took 0.28398 seconds\nFrame number 484 took 0.28656 seconds\nFrame number 485 took 0.28441 seconds\nFrame number 486 took 0.28336 seconds\nFrame number 487 took 0.28206 seconds\nFrame number 488 took 0.28605 seconds\nFrame number 489 took 0.28500 seconds\nFrame number 490 took 0.28582 seconds\nFrame number 491 took 0.28510 seconds\nFrame number 492 took 0.28411 seconds\nFrame number 493 took 0.28051 seconds\nFrame number 494 took 0.28275 seconds\nFrame number 495 took 0.28497 seconds\nFrame number 496 took 0.28270 seconds\nFrame number 497 took 0.29587 seconds\nFrame number 498 took 0.28226 seconds\nFrame number 499 took 0.28232 seconds\nFrame number 500 took 0.28405 seconds\nFrame number 501 took 0.28243 seconds\nFrame number 502 took 0.28481 seconds\nFrame number 503 took 0.28029 seconds\nFrame number 504 took 0.27936 seconds\nFrame number 505 took 0.28082 seconds\nFrame number 506 took 0.28182 seconds\nFrame number 507 took 0.28221 seconds\nFrame number 508 took 0.28344 seconds\nFrame number 509 took 0.28629 seconds\nFrame number 510 took 0.28458 seconds\nFrame number 511 took 0.28489 seconds\nFrame number 512 took 0.28251 seconds\nFrame number 513 took 0.28448 seconds\nFrame number 514 took 0.28538 seconds\nFrame number 515 took 0.28534 seconds\nFrame number 516 took 0.28393 seconds\nFrame number 517 took 0.28128 seconds\n\n\n🏁 FPS results\n\nprint('Total number of frames', f)\nprint('Total amount of time {:.5f} seconds'.format(t))\nprint('FPS:', round((f / t), 1))\n\n\n\nTotal number of frames 517\nTotal amount of time 152.90212 seconds\nFPS: 3.4\n\n\n# Saving locally without committing\nfrom IPython.display import FileLink\n\nFileLink('result.mp4')\n\n\n\nresult.mp4\n\n🎈 Reading input images\n\n# Reading image with OpenCV library\n# In this way image is opened already as numpy array\n# WARNING! OpenCV by default reads images in BGR format\n# image_BGR = cv2.imread('../input/videofortesting/traffic_sign.jpg')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_1.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_2.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_3.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_4.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_1.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_2.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_3.png')\nimage_BGR = cv2.imread('../input/videofortesting/ts_final_1.png')\n\n# Check point\n# Showing image shape\nprint('Image shape:', image_BGR.shape)  # tuple of (731, 1092, 3)\n\n# Getting spatial dimension of input image\nh, w = image_BGR.shape[:2]  # Slicing from tuple only first two elements\n\n# Check point\n# Showing height an width of image\nprint('Image height={0} and width={1}'.format(h, w))  # 731 1092\n\n\n\nImage shape: (720, 1280, 3)\nImage height=720 and width=1280\n\n\n🧿 Processing single image\n\n# Variable for counting total processing time\nt = 0\n\n# Blob from current frame\nblob = cv2.dnn.blobFromImage(image_BGR, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n\n# Forward pass with blob through output layers\nnetwork.setInput(blob)\nstart = time.time()\noutput_from_network = network.forward(layers_names_output)\nend = time.time()\n\n# Time\nt += end - start\nprint('Total amount of time {:.5f} seconds'.format(t))\n\n# Lists for detected bounding boxes, confidences and class's number\nbounding_boxes = []\nconfidences = []\nclass_numbers = []\n\n# Going through all output layers after feed forward pass\nfor result in output_from_network:\n    # Going through all detections from current output layer\n    for detected_objects in result:\n        # Getting 80 classes' probabilities for current detected object\n        scores = detected_objects[5:]\n        # Getting index of the class with the maximum value of probability\n        class_current = np.argmax(scores)\n        # Getting value of probability for defined class\n        confidence_current = scores[class_current]\n\n        # Eliminating weak predictions by minimum probability\n        if confidence_current &gt; probability_minimum:\n            # Scaling bounding box coordinates to the initial frame size\n            box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n            # Getting top left corner coordinates\n            x_center, y_center, box_width, box_height = box_current\n            x_min = int(x_center - (box_width / 2))\n            y_min = int(y_center - (box_height / 2))\n\n            # Adding results into prepared lists\n            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n            confidences.append(float(confidence_current))\n            class_numbers.append(class_current)\n                \n\n# Implementing non-maximum suppression of given bounding boxes\nresults = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n\n# Checking if there is any detected object been left\nif len(results) &gt; 0:\n    # Going through indexes of results\n    for i in results.flatten():\n        # Bounding box coordinates, its width and height\n        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n            \n            \n        # Cut fragment with Traffic Sign\n        c_ts = image_BGR[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n        # print(c_ts.shape)\n            \n        if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n            pass\n        else:\n            # Getting preprocessed blob with Traffic Sign of needed shape\n            blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n            blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n            blob_ts = blob_ts.transpose(0, 2, 3, 1)\n            # plt.imshow(blob_ts[0, :, :, :])\n            # plt.show()\n\n            # Feeding to the Keras CNN model to get predicted label among 43 classes\n            scores = model.predict(blob_ts)\n\n            # Scores is given for image with 43 numbers of predictions for each class\n            # Getting only one class with maximum value\n            prediction = np.argmax(scores)\n            print(labels['SignName'][prediction])\n\n\n            # Colour for current bounding box\n            colour_box_current = colours[class_numbers[i]].tolist()\n            \n            # Green BGR\n            colour_box_current = [0, 255, 61]\n            \n            # Yellow BGR\n#             colour_box_current = [0, 255, 255]\n\n            # Drawing bounding box on the original current frame\n            cv2.rectangle(image_BGR, (x_min, y_min),\n                              (x_min + box_width, y_min + box_height),\n                              colour_box_current, 6)\n\n#             # Preparing text with label and confidence for current bounding box\n#             text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n#                                                    confidences[i])\n            \n#             # Putting text with label and confidence on the original image\n#             cv2.putText(image_BGR, text_box_current, (x_min, y_min - 15),\n#                             cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n            \n            if prediction == 5:\n                # Preparing text with label and confidence for current bounding box\n                text_box_current = '{}: {:.4f}'.format('Speed limit 60', confidences[i])\n\n                # Putting text with label and confidence on the original image\n                cv2.putText(image_BGR, text_box_current, (x_min - 110, y_min - 10),\n                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n                \n            elif prediction == 9:            \n                # Preparing text with label and confidence for current bounding box\n                text_box_current = '{}: {:.4f}'.format('No overtaking', confidences[i])\n\n                # Putting text with label and confidence on the original image\n                cv2.putText(image_BGR, text_box_current, (x_min - 110, y_min + box_height + 30),\n                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n\n#             elif prediction == 17:            \n#                 # Preparing text with label and confidence for current bounding box\n#                 text_box_current = '{}: {:.4f}'.format('No entry', confidences[i])\n\n#                 # Putting text with label and confidence on the original image\n#                 cv2.putText(image_BGR, text_box_current, (x_min - 170, y_min - 15),\n#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n                \n                \n# Saving image\ncv2.imwrite('result.png', image_BGR)\n\n\n\nTotal amount of time 0.29843 seconds\nAhead only\nNo passing\n\n\n\n\n\nTrue\n\n\n🦞 Showing processed image\n\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (35.0, 35.0) # Setting default size of plots\n\nimage_BGR = cv2.imread('/kaggle/working/result.png')\n\n# Showing image shape\nprint('Image shape:', image_BGR.shape)  # tuple of (800, 1360, 3)\n\n# Getting spatial dimension of input image\nh, w = image_BGR.shape[:2]  # Slicing from tuple only first two elements\n\n# Showing height an width of image\nprint('Image height={0} and width={1}'.format(h, w))  # 800 1360\n\nplt.imshow(cv2.cvtColor(image_BGR, cv2.COLOR_BGR2RGB))\nplt.axis('off')\n# plt.title('Keras Visualization', fontsize=18)\n\n# Showing the plot\nplt.show()\n\nplt.close()\n\n\n\nImage shape: (720, 1280, 3)\nImage height=720 and width=1280\n\n\n\n\n# Saving locally without committing\nfrom IPython.display import FileLink\n\nFileLink('result.png')\n\n\n\nresult.png\n\n🔎 Example of the result\n\n\n",
        "url": "/ml_project/2021/03/22/traffic-signs-detection/"
      },
    
      {
        "title": "My journey from Java to Python",
        "excerpt": "In the fast-paced realm of software engineering, adaptability and versatility are key. Meet a results-driven Software Engineer with a diverse background spanning Finance, Product, and Agriculture domains. Armed with ~6 years of industry experience and a Master’s degree in Computer Science Engineering with a specialization in Data Science, this engineer has navigated through various roles, demonstrating expertise in both individual contributor and managerial capacities.\n\n",
        "content": "In the fast-paced realm of software engineering, adaptability and versatility are key. Meet a results-driven Software Engineer with a diverse background spanning Finance, Product, and Agriculture domains. Armed with ~6 years of industry experience and a Master’s degree in Computer Science Engineering with a specialization in Data Science, this engineer has navigated through various roles, demonstrating expertise in both individual contributor and managerial capacities.\n\n\n\n  Tech Odyssey: From Java to Python\n\n\nThis journey begins with a robust foundation in Java, honed during an enriching tenure at IBM, Inc. The engineer delved into the intricacies of enterprise-level applications, mastering standard programming practices, and embracing test-driven development (TDD) with Java as the backend. This experience laid the groundwork for a career marked by a commitment to excellence.\n\nAs the tech landscape evolved, so did the engineer’s toolkit. A pivotal point in the journey was the role as a Software Engineer at Syngenta, Inc., where the focus shifted to agriculture technology. Here, the engineer crafted a recommendation system for Syngenta’s patent on the Runoff and Leaching Algorithm. Using Python, Flask, Java, and AWS cloud services, they built REST APIs and processed data for millions of farmlands, contributing to a 12% improvement in business investment in Europe.\n\n\n  Versatility in the Clouds\n\n\nThe transition from Java to Python was seamless, evident in the Data Engineer Summer Internship at Robert Bosch, LLC. Here, the engineer orchestrated a Databricks cloud batch job, efficiently processing a million records in 24 hours. The scalable ETL pipeline, crafted in Python, Azure, Spark, and Databricks, showcased the engineer’s adaptability across diverse tech stacks.\n\n\n  Agile Innovations: Python, Django, and Beyond\n\n\nThe narrative unfolds further at Capco Technologies, Inc., where the engineer played a pivotal role in building a Travel Portal Application using Python, Django, and Java. The application streamlined travel plan approvals, significantly reducing latency. The tech stack expanded to encompass Python, Django, Java, MySQL, RESTful APIs, and Microservices, showcasing a versatile skill set.\n\n\n  Python as a Catalyst for Machine Learning\n\n\nThe final leg of the journey brings us to the academic realm, where the engineer, pursuing a Master’s degree at San Jose State University, delved into the world of Python for Machine Learning. Crafting a bot detection system with a random forest regression model, Python became the catalyst for real-time bid detection on auctioning sites, achieving an impressive prediction accuracy of 96%.\n\n\n  A Comprehensive Toolkit: Python, Java, and Beyond\n\n\nIn essence, the journey from Java to Python for this Software Engineer exemplifies a commitment to continuous learning and adaptability. From building enterprise-level applications in Java to orchestrating scalable ETL pipelines in Python, the engineer’s toolkit is comprehensive. Python, Java, Flask, Django, and a myriad of other technologies harmoniously coexist, creating a symphony of expertise that spans domains and enriches the tech landscape.\n\nAs this seasoned Software Engineer actively seeks new opportunities, the journey from Java to Python stands as a testament to the ever-evolving nature of the tech industry and the engineer’s ability to navigate and thrive in this dynamic landscape.\n",
        "url": "/history/professional%20experience/2021/04/28/example-post-two/"
      },
    
      {
        "title": "Story of brutal code refactoring",
        "excerpt": "In the dynamic realm of backend development, where the heartbeat of a system is measured in lines of code, there exists the occasional need for a revolutionary overhaul. This is the story of a relentless pursuit of excellence, a journey of agile code refactoring within the Python and Java tech stack.\n",
        "content": "In the dynamic realm of backend development, where the heartbeat of a system is measured in lines of code, there exists the occasional need for a revolutionary overhaul. This is the story of a relentless pursuit of excellence, a journey of agile code refactoring within the Python and Java tech stack.\n\nNestled within the digital corridors of a tech company, a once-efficient module had succumbed to the weight of evolving requirements and hurried fixes, transforming into a tangled web of complexity. Bugs lingered like ghosts of past implementations, demanding a radical transformation. It was against this backdrop that the engineer embarked on a mission to redefine the architecture.\n\nWith an astute understanding of the Python and Java tech stack, the engineer delved into the task of ruthless code refactoring, employing various techniques to reshape the codebase.\n\n\n  Extract Method:\nLike a surgeon separating conjoined code, the engineer used the Extract Method technique to break down complex functions into smaller, more manageable units. This not only enhanced readability but also facilitated easier debugging.\n  Introduce Design Patterns:\nDrawing inspiration from the vast repertoire of design patterns, the engineer introduced solutions like the Singleton Pattern and Factory Pattern to streamline the architecture. These patterns acted as building blocks, fostering a more cohesive and modular structure.\n  Replace Conditional with Polymorphism:\nIn instances where conditional statements had proliferated like weeds, the engineer leveraged polymorphism to replace complex conditionals with a more elegant and extensible structure. This not only simplified the code but also paved the way for future enhancements.\n  Refine Variable Names and Comments:\nLike a wordsmith perfecting a poem, the engineer combed through the code, refining variable names and updating comments. This not only improved code readability but also served as documentation for future developers navigating the system.\n  Implement Unit Tests:\nWith the commitment to fortify the system against future challenges, the engineer introduced a suite of unit tests using frameworks like JUnit in Java and pytest in Python. These tests became guardians, ensuring the stability and reliability of the revamped codebase.\n\n\nAs the refactor unfolded, the symphony of agile techniques played out, creating a harmonious balance between functionality and maintainability. The once-muddled module now stood as a testament to the transformative power of agile code refactoring in Python and Java.\n\nWord of the successful refactor reverberated through the development team, inspiring a renewed commitment to agile practices and clean coding. The blog of agile code refactoring became a beacon for developers, illustrating that even the most intricate code could be reshaped into an agile and responsive structure.\n\nAnd so, the symphony of agile code refactoring echoed through the digital corridors, leaving behind a legacy of innovation and efficiency in the ever-evolving landscape of backend development.\n",
        "url": "/general/2021/06/29/example-post-three/"
      },
    
      {
        "title": "Traffic Signs Classification",
        "excerpt": "Revolutionize road safety through Traffic Signs Classification with Convolutional Neural Networks (CNN). Tailored CNN architectures excel in accurately categorizing diverse traffic signs, leveraging their ability to capture spatial hierarchies and patterns. This technology is a pivotal force in enhancing road safety measures and supporting intelligent transportation systems.\n",
        "content": "Traffic Signs Classification with CNN\n\n\n  Implementing Traffic Signs Classification with Convolutional Neural Networks (CNN) represents a significant stride in computer vision applications. By utilizing CNN architectures tailored for image classification tasks, this approach excels in accurately categorizing various traffic signs. The inherent ability of CNNs to capture spatial hierarchies and patterns within images allows for robust recognition of diverse sign shapes, colors, and symbols. The model’s training involves learning intricate features, empowering it to distinguish between distinct classes, including speed limits, stop signs, and directional indicators. This technology serves as a pivotal component in enhancing road safety measures and supporting intelligent transportation systems.\n\n\nImporting needed libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\nimport matplotlib.pyplot as plt\nfrom timeit import default_timer as timer\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('archive/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nprint(os.listdir('archive/'))\n\n# Any results we write to the current directory are saved as output\n\n\n\narchive/test.pickle\narchive/data0.pickle\narchive/data2.pickle\narchive/std_gray.pickle\narchive/mean_image_rgb.pickle\narchive/data6.pickle\narchive/datasets_preparing.py\narchive/data8.pickle\narchive/data4.pickle\narchive/label_names.csv\narchive/data1.pickle\narchive/valid.pickle\narchive/std_rgb.pickle\narchive/data3.pickle\narchive/train.pickle\narchive/data7.pickle\narchive/mean_image_gray.pickle\narchive/data5.pickle\narchive/labels.pickle\n['test.pickle', 'data0.pickle', 'data2.pickle', 'std_gray.pickle', 'mean_image_rgb.pickle', 'data6.pickle', 'datasets_preparing.py', 'data8.pickle', 'data4.pickle', 'label_names.csv', 'data1.pickle', 'valid.pickle', 'std_rgb.pickle', 'data3.pickle', 'train.pickle', 'data7.pickle', 'mean_image_gray.pickle', 'data5.pickle', 'labels.pickle']\n\n\nLoading dataset data2.pickle with RGB examples\n\n# Opening file for reading in binary mode\nwith open('archive/data2.pickle', 'rb') as f:\n    data = pickle.load(f, encoding='latin1')  # dictionary type\n\n# Preparing y_train and y_validation for using in Keras\ndata['y_train'] = to_categorical(data['y_train'], num_classes=43)\ndata['y_validation'] = to_categorical(data['y_validation'], num_classes=43)\n\n# Making channels come at the end\ndata['x_train'] = data['x_train'].transpose(0, 2, 3, 1)\ndata['x_validation'] = data['x_validation'].transpose(0, 2, 3, 1)\ndata['x_test'] = data['x_test'].transpose(0, 2, 3, 1)\n\n# Showing loaded data from file\nfor i, j in data.items():\n    if i == 'labels':\n        print(i + ':', len(j))\n    else: \n        print(i + ':', j.shape)\n\n# x_train: (86989, 32, 32, 3)\n# y_train: (86989, 43)\n# x_test: (12630, 32, 32, 3)\n# y_test: (12630,)\n# x_validation: (4410, 32, 32, 3)\n# y_validation: (4410, 43)\n# labels: 43\n\n\n\ny_test: (12630,)\ny_validation: (4410, 43)\nx_validation: (4410, 32, 32, 3)\nx_train: (86989, 32, 32, 3)\ny_train: (86989, 43)\nlabels: 43\nx_test: (12630, 32, 32, 3)\n\n\nShowing some examples\n\n%matplotlib inline\n\n# Preparing function for ploting set of examples\n# As input it will take 4D tensor and convert it to the grid\n# Values will be scaled to the range [0, 255]\ndef convert_to_grid(x_input):\n    N, H, W, C = x_input.shape\n    grid_size = int(np.ceil(np.sqrt(N)))\n    grid_height = H * grid_size + 1 * (grid_size - 1)\n    grid_width = W * grid_size + 1 * (grid_size - 1)\n    grid = np.zeros((grid_height, grid_width, C)) + 255\n    next_idx = 0\n    y0, y1 = 0, H\n    for y in range(grid_size):\n        x0, x1 = 0, W\n        for x in range(grid_size):\n            if next_idx &lt; N:\n                img = x_input[next_idx]\n                low, high = np.min(img), np.max(img)\n                grid[y0:y1, x0:x1] = 255.0 * (img - low) / (high - low)\n                next_idx += 1\n            x0 += W + 1\n            x1 += W + 1\n        y0 += H + 1\n        y1 += H + 1\n\n    return grid\n\n\n# Visualizing some examples of training data\nexamples = data['x_train'][:81, :, :, :]\nprint(examples.shape)  # (81, 32, 32, 3)\n\n# Plotting some examples\nfig = plt.figure()\ngrid = convert_to_grid(examples)\nplt.imshow(grid.astype('uint8'), cmap='gray')\nplt.axis('off')\nplt.gcf().set_size_inches(15, 15)\nplt.title('Some examples of training data', fontsize=18)\n\n# Showing the plot\nplt.show()\n\n# Saving the plot\nfig.savefig('training_examples.png')\nplt.close()\n\n\n\n(81, 32, 32, 3)\n\n\n\n\nBuilding model of CNN with Keras\nTrying one model with filters of size 3x3\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=3, padding='same', activation='relu', input_shape=(32, 32, 3)))\nmodel.add(MaxPool2D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dense(43, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\n2022-05-08 23:13:33.413998: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nOverfitting the 3x3 model with small amount of data\n\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs))\nepochs = 15\n\nh = model.fit(data['x_train'][:10], data['y_train'][:10],\n              batch_size=5, epochs = epochs,\n              validation_data = (data['x_validation'], data['y_validation']),\n              callbacks=[annealer], verbose=1)\n\n\n\nEpoch 1/15\n2/2 [==============================] - 3s 2s/step - loss: 3.7969 - accuracy: 0.0000e+00 - val_loss: 3.8041 - val_accuracy: 0.0333 - lr: 4.6329e-04\nEpoch 2/15\n2/2 [==============================] - 2s 2s/step - loss: 3.1586 - accuracy: 0.4000 - val_loss: 3.9762 - val_accuracy: 0.0408 - lr: 4.4013e-04\nEpoch 3/15\n2/2 [==============================] - 2s 2s/step - loss: 2.5038 - accuracy: 0.4000 - val_loss: 4.3737 - val_accuracy: 0.0397 - lr: 4.1812e-04\nEpoch 4/15\n2/2 [==============================] - 2s 2s/step - loss: 2.0482 - accuracy: 0.5000 - val_loss: 4.8843 - val_accuracy: 0.0415 - lr: 3.9721e-04\nEpoch 5/15\n2/2 [==============================] - 2s 2s/step - loss: 1.7053 - accuracy: 0.7000 - val_loss: 5.4301 - val_accuracy: 0.0435 - lr: 3.7735e-04\nEpoch 6/15\n2/2 [==============================] - 2s 2s/step - loss: 1.4073 - accuracy: 0.8000 - val_loss: 5.9822 - val_accuracy: 0.0399 - lr: 3.5849e-04\nEpoch 7/15\n2/2 [==============================] - 2s 2s/step - loss: 1.1944 - accuracy: 0.8000 - val_loss: 6.5181 - val_accuracy: 0.0390 - lr: 3.4056e-04\nEpoch 8/15\n2/2 [==============================] - 2s 2s/step - loss: 1.0363 - accuracy: 0.8000 - val_loss: 6.9806 - val_accuracy: 0.0392 - lr: 3.2353e-04\nEpoch 9/15\n2/2 [==============================] - 2s 2s/step - loss: 0.9091 - accuracy: 1.0000 - val_loss: 7.3931 - val_accuracy: 0.0444 - lr: 3.0736e-04\nEpoch 10/15\n2/2 [==============================] - 2s 2s/step - loss: 0.7958 - accuracy: 1.0000 - val_loss: 7.7534 - val_accuracy: 0.0447 - lr: 2.9199e-04\nEpoch 11/15\n2/2 [==============================] - 2s 2s/step - loss: 0.7066 - accuracy: 1.0000 - val_loss: 8.0655 - val_accuracy: 0.0465 - lr: 2.7739e-04\nEpoch 12/15\n2/2 [==============================] - 2s 2s/step - loss: 0.6203 - accuracy: 1.0000 - val_loss: 8.3495 - val_accuracy: 0.0463 - lr: 2.6352e-04\nEpoch 13/15\n2/2 [==============================] - 2s 2s/step - loss: 0.5796 - accuracy: 1.0000 - val_loss: 8.5997 - val_accuracy: 0.0392 - lr: 2.5034e-04\nEpoch 14/15\n2/2 [==============================] - 2s 2s/step - loss: 0.5180 - accuracy: 1.0000 - val_loss: 8.8193 - val_accuracy: 0.0392 - lr: 2.3783e-04\nEpoch 15/15\n2/2 [==============================] - 2s 2s/step - loss: 0.4804 - accuracy: 1.0000 - val_loss: 9.0036 - val_accuracy: 0.0392 - lr: 2.2594e-04\n\n\nprint('Epochs={0:d}, training accuracy={1:.5f}, validation accuracy={2:.5f}'.\\\n      format(epochs, max(h.history['accuracy']), max(h.history['val_accuracy'])))\n\n\n\nEpochs=15, training accuracy=1.00000, validation accuracy=0.04649\n\n\nPlotting history results for overfitting small data\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (15.0, 5.0) # Setting default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['font.family'] = 'Times New Roman'\n\nfig = plt.figure()\nplt.plot(h.history['accuracy'], '-o', linewidth=3.0)\nplt.plot(h.history['val_accuracy'], '-o', linewidth=3.0)\nplt.title('Overfitting small data', fontsize=22)\nplt.legend(['train', 'validation'], loc='upper left', fontsize='xx-large')\nplt.xlabel('Epoch', fontsize=20)\nplt.ylabel('Accuracy', fontsize=20)\nplt.tick_params(labelsize=18)\n\n# Showing the plot\nplt.show()\n\n# Saving the plot\nfig.savefig('overfitting_small_data.png')\nplt.close()\n\n\n\n\n\nBuilding set of models of CNN with Keras\nTrying different models with different dimensions of filters\n\nfilters = [3, 5, 9, 13, 15, 19, 23, 25, 31]\nmodel = [0] * len(filters)\n\nfor i in range(len(model)):\n    model[i] = Sequential()\n    model[i].add(Conv2D(32, kernel_size=filters[i], padding='same', activation='relu', input_shape=(32, 32, 3)))\n    model[i].add(MaxPool2D(pool_size=2))\n    model[i].add(Flatten())\n    model[i].add(Dense(500, activation='relu'))\n    model[i].add(Dense(43, activation='softmax'))\n    model[i].compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\nTraining set of models of CNN with Keras\nAnd with different dimensions of filters\n\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs))\nepochs = 5\n\nh = [0] * len(model)\n\nfor i in range(len(h)):\n    h[i] = model[i].fit(data['x_train'], data['y_train'],\n                        batch_size=5, epochs = epochs,\n                        validation_data = (data['x_validation'], data['y_validation']),\n                        callbacks=[annealer], verbose=0)\n    \n    print('Model with filters {0:d}x{0:d}, epochs={1:d}, training accuracy={2:.5f}, validation accuracy={3:.5f}'.\\\n      format(filters[i], epochs, max(h[i].history['accuracy']), max(h[i].history['val_accuracy'])))\n\n\n\nModel with filters 3x3, epochs=5, training accuracy=0.98907, validation accuracy=0.87392\nModel with filters 5x5, epochs=5, training accuracy=0.98709, validation accuracy=0.88073\n\n\nPlotting comparison results for accuracy\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (15.0, 15.0) # Setting default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['font.family'] = 'Times New Roman'\n\n# Plotting history of training accuracy\nfig = plt.figure()\nplt.subplot(2, 1, 1)\nplt.plot(h[8].history['accuracy'], '-o', linewidth=3.0)\nplt.plot(h[7].history['accuracy'], '-s', linewidth=3.0)\nplt.plot(h[6].history['accuracy'], '-D', linewidth=3.0)\nplt.plot(h[5].history['accuracy'], '-D', linewidth=3.0)\nplt.plot(h[4].history['accuracy'], '-o', linewidth=3.0)\nplt.plot(h[3].history['accuracy'], '-o', linewidth=3.0)\nplt.plot(h[2].history['accuracy'], '-o', linewidth=3.0)\nplt.plot(h[1].history['accuracy'], '-o', linewidth=3.0)\nplt.plot(h[0].history['accuracy'], '-o', linewidth=3.0)\nplt.legend(['filter 31', 'filter 25', 'filter 23', 'filter 19', 'filter 15', 'filter 13', 'filter 9', 'filter 5', 'filter 3'], loc='lower right', fontsize='xx-large', borderpad=2)\nplt.xlabel('Epoch', fontsize=20, fontname='Times New Roman')\nplt.ylabel('Training Accuracy', fontsize=20, fontname='Times New Roman')\nplt.yscale('linear')  # {\"linear\", \"log\", \"symlog\", \"logit\", ...}\nplt.ylim(0.85, 1.0)\nplt.xlim(0.5, 5.3) \nplt.title('Accuracy for different sizes of filters', fontsize=22)\nplt.tick_params(labelsize=18)\n\nplt.subplot(2, 1, 2)\n# plt.gca().set_title('Validation accuracy')\nplt.plot(h[8].history['val_accuracy'], '-o', linewidth=3.0)\nplt.plot(h[7].history['val_accuracy'], '-s', linewidth=3.0)\nplt.plot(h[6].history['val_accuracy'], '-D', linewidth=3.0)\nplt.plot(h[5].history['val_accuracy'], '-D', linewidth=3.0)\nplt.plot(h[4].history['val_accuracy'], '-o', linewidth=3.0)\nplt.plot(h[3].history['val_accuracy'], '-o', linewidth=3.0)\nplt.plot(h[2].history['val_accuracy'], '-o', linewidth=3.0)\nplt.plot(h[1].history['val_accuracy'], '-o', linewidth=3.0)\nplt.plot(h[0].history['val_accuracy'], '-o', linewidth=3.0)\nplt.legend(['filter 31', 'filter 25', 'filter 23', 'filter 19', 'filter 15', 'filter 13', 'filter 9', 'filter 5', 'filter 3'], loc='lower right', fontsize='xx-large', borderpad=2)\nplt.xlabel('Epoch', fontsize=20, fontname='Times New Roman')\nplt.ylabel('Validation Accuracy', fontsize=20, fontname='Times New Roman')\nplt.yscale('linear')  # {\"linear\", \"log\", \"symlog\", \"logit\", ...}\nplt.ylim(0.75, 0.9)\nplt.xlim(0.5, 5.3)\nplt.tick_params(labelsize=18)\n\n# Showing the plot\nplt.show()\n\n# Saving the plot\nfig.savefig('models_accuracy.png')\nplt.close()\n\n\n# Showing values of accuracy for different filters\nfor i in range(len(h)):\n    print('data2 filter {0:d} training accuracy = {1:.5f}'.\\\n          format(filters[i], np.max(h[i].history['acc'])))\n\nprint()\n\nfor i in range(len(h)):\n    print('data2 filter {0:d} validation accuracy = {1:.5f}'.\\\n          format(filters[i], np.max(h[i].history['val_acc'])))\n\n\n\nCalculating accuracy with testing dataset\n\nfor i in range(len(model)):\n    temp = model[i].predict(data['x_test'])\n    temp = np.argmax(temp, axis=1)\n\n    # We compare predicted class with correct class for all input images\n    # And calculating mean value among all values of following numpy array\n    # By saying 'testing_accuracy == data['y_test']' we create numpy array with True and False values\n    # 'np.mean' function will return average of the array elements\n    # The average is taken over the flattened array by default\n    temp = np.mean(temp == data['y_test'])\n    \n    print('data2 filter {0:d} testing accuracy = {1:.5f}'.format(filters[i], temp))\n\n\n\nTime for classification\n\n# Getting scores from forward pass of one input image\n# Scores are given for each image with 43 numbers of predictions for each class\n# Measuring at the same time execution time\n\nfor i in range(len(model)):\n    start = timer()\n    temp = model[i].predict(data['x_test'][:1, :, :, :])\n    end = timer()\n    \n    print('data2 filter {0:d} classification time = {1:.5f}'.format(filters[i], end - start))\n\n\n\ndata2 filter 3 classification time = 0.01406\ndata2 filter 5 classification time = 0.00414\ndata2 filter 9 classification time = 0.00337\ndata2 filter 13 classification time = 0.00332\ndata2 filter 15 classification time = 0.00321\ndata2 filter 19 classification time = 0.00385\ndata2 filter 23 classification time = 0.00483\ndata2 filter 25 classification time = 0.00474\ndata2 filter 31 classification time = 0.01855\n\n\nVisualizing filters of convolutional layer\n\nfor i in range(len(model)):\n    w = model[i].get_weights()\n    print(w[0].shape)\n    # print(model[i].get_config())\n    # l = model[i].layers\n    # print(l[0].get_weights()[0].shape)\n\n    # Visualizing filters\n    temp = w[0].transpose(3, 0, 1, 2)\n    print(temp.shape)  # (81, 32, 32, 3)\n\n    # Plotting\n    fig = plt.figure()\n    grid = convert_to_grid(temp)\n    plt.imshow(grid.astype('uint8'), cmap='gray')\n    plt.axis('off')\n    plt.gcf().set_size_inches(10, 10)\n    name = 'Trained filters ' + str(filters[i]) + 'x' + str(filters[i])\n    plt.title(name, fontsize=18)\n    \n    # Showing the plot\n    plt.show()\n\n    # Saving the plot\n    name = 'filters-' + str(filters[i]) + 'x' + str(filters[i]) + '.png'\n    fig.savefig(name)\n    plt.close()\n\n\n\n(3, 3, 3, 32)\n(32, 3, 3, 3)\n\n\n\n\n(5, 5, 3, 32)\n(32, 5, 5, 3)\n\n\n\n\n(9, 9, 3, 32)\n(32, 9, 9, 3)\n\n\n\n\n(13, 13, 3, 32)\n(32, 13, 13, 3)\n\n\n\n\n(15, 15, 3, 32)\n(32, 15, 15, 3)\n\n\n\n\n(19, 19, 3, 32)\n(32, 19, 19, 3)\n\n\n\n\n(23, 23, 3, 32)\n(32, 23, 23, 3)\n\n\n\n\n(25, 25, 3, 32)\n(32, 25, 25, 3)\n\n\n\n\n(31, 31, 3, 32)\n(32, 31, 31, 3)\n\n\n\n\nPredicting with one image from test dataset\n\n%matplotlib inline\n\n# Preparing image for predicting from test dataset\nx_input = data['x_test'][100:101]\nprint(x_input.shape)\ny_input = data['y_test'][100:101]\nprint(y_input)\n\nplt.rcParams['figure.figsize'] = (2.5, 2.5) # Setting default size of plots\nplt.imshow(x_input[0, :, :, :])\nplt.axis('off')\n\n# Showing the plot\nplt.show()\n\n# Getting scores from forward pass of input image\nscores = model[0].predict(x_input)\nprint(scores[0].shape) # (43,)\n\n# Scores is given for image with 43 numbers of predictions for each class\n# Getting only one class with maximum value\nprediction = np.argmax(scores)\nprint('ClassId:', prediction)\n\n# Defining function for getting texts for every class - labels\ndef label_text(file):\n    # Defining list for saving label in order from 0 to 42\n    label_list = []\n    \n    # Reading 'csv' file and getting image's labels\n    r = pd.read_csv(file)\n    # Going through all names\n    for name in r['SignName']:\n        # Adding from every row second column with name of the label\n        label_list.append(name)\n    \n    # Returning resulted list with labels\n    return label_list\n\n\n# Getting labels\nlabels = label_text('archive/label_names.csv')\n\n# Printing label for classified Traffic Sign\nprint('Label:', labels[prediction])\n\n\n\n(1, 32, 32, 3)\n[3]\n\n\n\n\n(43,)\nClassId: 3\nLabel: Speed limit (60km/h)\n\n\nSaving models\n\nfor i in range(len(model)):\n    name = 'model-' + str(filters[i]) + 'x' + str(filters[i]) + '.h5'\n    model[i].save(name)\n\n# # Saving model locally without committing\n# from IPython.display import FileLink\n\n# FileLink('model-3x3.h5')\n\n\n",
        "url": "/ml_experiment/2022/07/22/traffic-signs-classification-with-cnn/"
      },
    
      {
        "title": "Optimizing Kubernetes - Unveiling Solutions to the 'Cold-Start",
        "excerpt": "In the intricate world of containerized applications, the persistent challenge of the “cold-start” problem has spurred innovative minds to delve deep into the heart of Kubernetes clusters. This blog unveils the journey of a dedicated engineer who, with a keen focus on latency issues, has not only identified but significantly alleviated performance bottlenecks within Kubernetes environments.\n\n",
        "content": "In the intricate world of containerized applications, the persistent challenge of the “cold-start” problem has spurred innovative minds to delve deep into the heart of Kubernetes clusters. This blog unveils the journey of a dedicated engineer who, with a keen focus on latency issues, has not only identified but significantly alleviated performance bottlenecks within Kubernetes environments.\n\n\n\n\n  Decoding the ‘Cold-Start’ Challenge\n\n\nThe first chapter of this narrative revolves around addressing the notorious “cold-start” problem that plagues containerized applications. Through meticulous analysis of latency issues within Kubernetes clusters, our intrepid engineer identified key pain points and embarked on a mission to revolutionize performance.\n\n\n  The Art of Optimization: Container Preload, Caching, and Eviction\n\n\nA pivotal moment in this journey was the groundbreaking achievement of vastly improved performance through the optimization of container preload, caching, and eviction. By fine-tuning these elements based on the specific request patterns of machine learning applications, our engineer paved the way for a more responsive and efficient Kubernetes ecosystem.\n\n\n  Innovation Unleashed: K3s Kubernetes Application Framework\n\n\nThe narrative unfolds further with the development of a K3s Kubernetes-based application framework designed specifically to address the unique challenges posed by mobile edge computing environments. This innovative framework not only hosts a diverse array of machine learning applications but also hones in on predicting client mobility while simultaneously optimizing container latency.\n\n\n  A Triumph on the K8s Stage: Boosting Performance to 60%\n\n\nThe climax of this story takes place on a 3-node Kubernetes (K8s) cluster, where our engineer demonstrated unparalleled success. By hosting machine learning applications with an impressive 20 replicas each, the performance soared to a remarkable 60%. This feat not only surpassed expectations but outshone traditional cloud platforms, marking a significant milestone in the ongoing battle against latency in machine learning applications.\n\nAs we conclude this journey into the intricacies of Kubernetes optimization for machine learning, it becomes evident that our dedicated engineer’s innovative solutions have far-reaching implications. The quest to conquer the “cold-start” problem has not only enhanced the performance of containerized applications but has also paved the way for future advancements in the dynamic landscape of Kubernetes and machine learning.\n\nIn the ever-evolving realm of technology, where challenges breed innovation, this blog serves as a testament to the relentless pursuit of excellence. Our engineer’s journey from analyzing latency issues to optimizing container behaviors reflects the spirit of progress that defines the Kubernetes ecosystem, promising a brighter and more efficient future for containerized applications in the world of machine learning.\n\n",
        "url": "/research/professional%20project/2023/09/27/example-post-one/"
      },
    
  
    
    
  
  
  
  {
    "title": "Categories",
    "excerpt": "Category index\n",
    "content": "\n",
    "url": "/categories/"
  },
  
  {
    "title": "Projects",
    "excerpt": "A demo of Markdown and HTML includes\n",
    "content": "Heading 1\n\nHeading 2\n\nHeading 3\n\nHeading 4\n\nHeading 5\n\nHeading 6\n\nA small project\n\nA link\n\nLorem ipsum dolor sit amet, consectetur adip* isicing elit, sed do eiusmod *tempor incididunt ut labore et dolore magna aliqua.\n\nDuis aute irure dolor in A link reprehenderit in voluptate velit esse cillum bold text dolore eu fugiat nulla pariatur. Excepteur span element sint occaecat cupidatat non proident, sunt italicised text in culpa qui officia deserunt mollit anim id some code est laborum.\n\n\n  An item\n  An item\n  An item\n  An item\n  An item\n\n\n\n  Item one\n  Item two\n  Item three\n  Item four\n  Item five\n\n\n\n  A simple blockquote\n\n\nSome HTML…\n\n&lt;blockquote cite=\"http://www.imdb.com/title/tt0284978/quotes/qt1375101\"&gt;\n  &lt;p&gt;You planning a vacation, Mr. Sullivan?&lt;/p&gt;\n  &lt;footer&gt;\n    &lt;a href=\"http://www.imdb.com/title/tt0284978/quotes/qt1375101\"&gt;Sunways Security Guard&lt;/a&gt;\n  &lt;/footer&gt;\n&lt;/blockquote&gt;\n\n\n…CSS…\n\nblockquote {\n  text-align: center;\n  font-weight: bold;\n}\nblockquote footer {\n  font-size: .8rem;\n}\n\n\n…and JavaScript\n\nconst blockquote = document.querySelector(\"blockquote\")\nconst bolden = (keyString, string) =&gt;\n  string.replace(new RegExp(keyString, 'g'), '&lt;strong&gt;'+keyString+'&lt;/strong&gt;')\n\nblockquote.innerHTML = bolden(\"Mr. Sullivan\", blockquote.innerHTML)\n\n\nSingle line of code\n\nHTML Includes\n\nContact form\n\n\n  \n    Contact\n    Name: *\n    \n\n    Email Address: *\n    \n\n    Message: *\n    \n\n    \n    \n    * indicates a required field\n\n    \n      \n      \n      \n    \n  \n\n\n\n\nPlease enable JavaScript to use the form.\n\n{% include site-form.html %}\n\n\nDemo map embed\n\n\n\n{% include map.html id=\"XXXXXX\" title=\"Coffee shop map\" %}\n\n\nButton include\n\nA button\n\nA button with icon  twitter\n\n\n{% include button.html text=\"A button\" link=\"https://david.darn.es\" %}\n{% include button.html text=\"A button with icon\" link=\"https://twitter.com/daviddarnes\" icon=\"twitter\" %}\n\n\nIcon include\n\ntwitter\ntwitter\n\n\n{% include icon.html id=\"twitter\" title=\"twitter\" %}\n[{% include icon.html id=\"linkedin\" title=\"twitter\" %}](https://www.linkedin.com/in/sonali-thote)\n\n\nVideo include\n\n\n  \n\n\n{% include video.html id=\"zrkcGL5H3MU\" title=\"Siteleaf tutorial video\" %}\n\n\nImage includes\n\n\n  \n  Image with caption\n\n\n\n  \n  Right aligned image\n\n\n\n  \n  Left aligned image\n\n\n\n  \n  \n\n\n{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Image with caption\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Right aligned image\" position=\"right\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Left aligned image\" position=\"left\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/1600/800?image=894\" alt=\"Image with just alt text\" %}\n\n",
    "url": "/elements/"
  },
  
  {
    "title": "About me",
    "excerpt": "Software Engineer\n",
    "content": "As a seasoned Software Engineer with a rich background spanning over six years, my career has traversed through various sectors including Finance, Product, and Agriculture, where I’ve excelled in crafting scalable and highly available systems. My technical proficiency is matched by my capability to lead and manage, ensuring that every project under my stewardship reaches its full potential.\n\nMy academic journey culminated in earning a Master’s degree in Computer Science Engineering with a specialization in Data Science from San Jose State University, California. This solid educational foundation has been pivotal in shaping my approach to solving complex problems and innovating within the tech landscape.\n\nCurrently residing in San Jose, California, I am on the lookout for new opportunities that can benefit from my extensive experience and passion for technology. I am eager to bring my blend of leadership, technical expertise, and innovative thinking to a dynamic team driving forward in the Bay Area.\n\nConnect Catalyst   linkedin\n\n Code Canvas  github\n\n Tweet me  twitter\n\n\n\n\n\n",
    "url": "/"
  },
  
  {
    "title": "Kubernetes in a nutshell",
    "excerpt": "\n",
    "content": "The complete distilling apparatus consists of three parts: the “cucurbit” (Arabic ḳarʿa, Greek βίκος), the still pot containing the liquid to be distilled, which is heated by a flame; the “head” or “cap” (Arabic anbiḳ, Greek ἄμβιξ) which fits over the mouth of the cucurbit to receive the vapors, with an attached downward-sloping “tube” (Greek σωλήν), leading to the “receiver” (Arabic ḳābila, Greek ἄγγος or φιάλη) container.\n\n\n\nRetorts have the “cap” and the “cucurbit” made into one. The anbik is also called the raʾs (head) of the cucurbit. The liquid in the cucurbit is heated or boiled; the vapour rises into the anbik, where it cools by contact with the walls and condenses, running down the spout into the receiver. A modern descendant of the alembic is the pot still, used to produce distilled beverages.\n",
    "url": "/projects/project1/"
  },
  
  {
    "title": "Search",
    "excerpt": "Search for a page or post you’re looking for\n",
    "content": "{% include site-search.html %}\n",
    "url": "/search/"
  }
  
]

